{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "class XLMR:\n",
    "    def __init__(self):\n",
    "        self.model_name = 'xlmr'\n",
    "        self.nlp = pipeline(task =\"feature-extraction\", model = 'xlm-roberta-base', tokenizer='xlm-roberta-base', framework='pt', device=0)\n",
    "\n",
    "    def GetFeatures(self, sentences=None):\n",
    "        if self.model_name == 'xlmr':\n",
    "            features = self.nlp(sentences, truncation=True) \n",
    "            # featurelist=list()\n",
    "            # for i in features:\n",
    "            #    featurelist.append(i[0][0])\n",
    "            # self.features=pd.DataFrame(featurelist)        \n",
    "        return features[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlmr = XLMR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomBERTModel(nn.Module):\n",
    "    def __init__(self):\n",
    "          super(CustomBERTModel, self).__init__()\n",
    "     \n",
    "          self.linear1 = nn.Linear(1536, 768)\n",
    "          self.linear2 = nn.Linear(768, 300)\n",
    "          self.linear3 = nn.Linear(1068, 768)\n",
    "          self.linear4 = nn.Linear(768,2)\n",
    "          self.outputlayer = nn.Softmax(dim=1)\n",
    "\n",
    "    def concatenate(self, tensor1, tensor2):\n",
    "      return torch.cat((tensor1, tensor2), 1)\n",
    "      \n",
    "    def forward(self, post=None, context=None, text=None, only_post = False, post_context=False, post_context_text=False):\n",
    "\n",
    "      if only_post:\n",
    "        input_post_embeddings = post\n",
    "        \n",
    "        outputs = input_post_embeddings\n",
    "\n",
    "      if post_context:\n",
    "\n",
    "        input_post_embeddings = post\n",
    "\n",
    "        input_context_embeddings = context\n",
    "\n",
    "        post_context = self.concatenate(input_post_embeddings, input_context_embeddings)\n",
    "\n",
    "        linear1_output = self.linear1(post_context)\n",
    "\n",
    "        outputs = linear1_output\n",
    "\n",
    "      if post_context_text:\n",
    "\n",
    "        input_post_embeddings = post\n",
    "\n",
    "        input_context_embeddings = context\n",
    "\n",
    "        input_text_embeddings = text\n",
    "\n",
    "        context_text = self.concatenate(input_context_embeddings, input_post_embeddings)\n",
    "\n",
    "        linear1_output = self.linear1(context_text)\n",
    "\n",
    "        linear2_output = self.linear2(linear1_output)\n",
    "\n",
    "        post_context_text = self.concatenate(input_text_embeddings, linear2_output)\n",
    "\n",
    "        linear3_output = self.linear3(post_context_text)\n",
    "\n",
    "        outputs = linear3_output\n",
    "\n",
    "      return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomBERTModel()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"rev_xlmr_model_adamw.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_pickle(\"train_hierarchial.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "\n",
    "for post, context,text, label in zip(train[\"post\"], train[\"context\"], train[\"text\"], train[\"label\"]):\n",
    "\n",
    "    if post!=\"\" and context==\"\" and text==\"\":\n",
    "\n",
    "        input_post=xlmr.GetFeatures(post)\n",
    "        input_post = torch.FloatTensor([input_post]).to(device)\n",
    "\n",
    "        X.append(model(input_post, only_post=True))\n",
    "        y.append(label)\n",
    "    \n",
    "\n",
    "    if post!=\"\" and context!=\"\" and text==\"\":\n",
    "\n",
    "        input_post=xlmr.GetFeatures(post)\n",
    "        input_post = torch.FloatTensor([input_post]).to(device)\n",
    "\n",
    "        input_context=xlmr.GetFeatures(context)\n",
    "        input_context = torch.FloatTensor([input_context]).to(device)\n",
    "\n",
    "        X.append(model(input_post, input_context,post_context=True))\n",
    "        y.append(label)\n",
    "\n",
    "\n",
    "    if post!=\"\" and context!=\"\" and text!=\"\":\n",
    "\n",
    "        input_post=xlmr.GetFeatures(post)\n",
    "        input_post = torch.Tensor([input_post]).to(device)\n",
    "\n",
    "        input_context=xlmr.GetFeatures(context)\n",
    "        input_context = torch.Tensor([input_context]).to(device)\n",
    "\n",
    "        input_text=xlmr.GetFeatures(text)\n",
    "        input_text = torch.Tensor([input_text]).to(device)\n",
    "\n",
    "        X.append(model(input_post, input_context,input_text, post_context_text=True))\n",
    "        y.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.DataFrame([feat.to(\"cpu\").tolist()[0] for feat in X][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_X = x.values\n",
    "train_y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_pickle(\"test_hierarchial.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = []\n",
    "test_y = []\n",
    "\n",
    "\n",
    "for post, context,text, label in zip(test[\"post\"], test[\"context\"], test[\"text\"], test[\"label\"]):\n",
    "\n",
    "    if post!=\"\" and context==\"\" and text==\"\":\n",
    "\n",
    "        input_post=xlmr.GetFeatures(post)\n",
    "        input_post = torch.FloatTensor([input_post]).to(device)\n",
    "\n",
    "        test_X.append(model(input_post, only_post=True))\n",
    "        test_y.append(label)\n",
    "    \n",
    "\n",
    "    if post!=\"\" and context!=\"\" and text==\"\":\n",
    "\n",
    "        input_post=xlmr.GetFeatures(post)\n",
    "        input_post = torch.FloatTensor([input_post]).to(device)\n",
    "\n",
    "        input_context=xlmr.GetFeatures(context)\n",
    "        input_context = torch.FloatTensor([input_context]).to(device)\n",
    "\n",
    "        test_X.append(model(input_post, input_context,post_context=True))\n",
    "        test_y.append(label)\n",
    "\n",
    "\n",
    "    if post!=\"\" and context!=\"\" and text!=\"\":\n",
    "\n",
    "        input_post=xlmr.GetFeatures(post)\n",
    "        input_post = torch.Tensor([input_post]).to(device)\n",
    "\n",
    "        input_context=xlmr.GetFeatures(context)\n",
    "        input_context = torch.Tensor([input_context]).to(device)\n",
    "\n",
    "        input_text=xlmr.GetFeatures(text)\n",
    "        input_text = torch.Tensor([input_text]).to(device)\n",
    "\n",
    "        test_X.append(model(input_post, input_context,input_text, post_context_text=True))\n",
    "        test_y.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = pd.DataFrame([feat.to(\"cpu\").tolist()[0] for feat in test_X][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = test_x.values\n",
    "test_y = np.array(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_xgboost = {\"n_estimators\": 500, \"learning_rate\": 1, \"max_depth\": 2,\n",
    "                  \"objective\": 'multi:softmax', \"num_class\": 4}\n",
    "params_lr = {\"multi_class\": 'multinomial'}\n",
    "params_rf = {\"n_estimators\": 500, \"criterion\": 'entropy', \"random_state\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "class Classifiers:\n",
    "    def __init__(self, train_X, train_y, test_X, test_y):\n",
    "        self.Createstaticsplit(train_X, train_y, test_X, test_y)\n",
    "        self.accuracy = list()\n",
    "        self.f1score = list()\n",
    "\n",
    "    def MajorityVotingClassifier(self, num_class):\n",
    "        _, _, y_xgboost = self.XGBClassifier(num_class)\n",
    "        _, _, y_rf = self.RandomForestClassifier()\n",
    "        _, _, y_lr = self.LogisticRegression()\n",
    "        y_pred = list()\n",
    "        for i in range(len(y_xgboost)):\n",
    "            preds = list()\n",
    "            preds.append(y_xgboost[i])\n",
    "            preds.append(y_rf[i])\n",
    "            preds.append(y_lr[i])\n",
    "            y_pred.append(max(set(preds), key=preds.count))\n",
    "\n",
    "        with open('p2_predictions.pkl', 'wb') as f:\n",
    "          pickle.dump(y_pred, f)\n",
    "\n",
    "        return accuracy_score(\n",
    "            self.test_labels, y_pred), f1_score(\n",
    "            self.test_labels, y_pred, average='weighted')\n",
    "\n",
    "    def XGBClassifier(self, num_class):\n",
    "        classifier = XGBClassifier(**params_xgboost)\n",
    "        classifier.fit(self.train_features, self.train_labels)\n",
    "        y_pred = classifier.predict(self.test_features)\n",
    "        return accuracy_score(self.test_labels, y_pred), f1_score(self.test_labels, y_pred, average='weighted'), y_pred\n",
    "\n",
    "    def RandomForestClassifier(self):\n",
    "        classifier = RandomForestClassifier(**params_rf)\n",
    "        classifier.fit(self.train_features, self.train_labels)\n",
    "        y_pred = classifier.predict(self.test_features)\n",
    "        return accuracy_score(self.test_labels, y_pred), f1_score(self.test_labels, y_pred, average='weighted'), y_pred\n",
    "\n",
    "    def Createstaticsplit(self, train_X, train_y, test_X, test_y):\n",
    "        self.train_features = train_X\n",
    "        self.train_labels = train_y\n",
    "        self.test_features = test_X\n",
    "        self.test_labels = test_y\n",
    "\n",
    "    def LogisticRegression(self):\n",
    "        lr_clf = LogisticRegression(**params_lr)\n",
    "        lr_clf.fit(self.train_features, self.train_labels)\n",
    "        y_pred = lr_clf.predict(self.test_features)\n",
    "        return accuracy_score(self.test_labels, y_pred), f1_score(self.test_labels, y_pred, average='weighted'), y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = Classifiers(train_X, train_y, test_X, test_y)\n",
    "\n",
    "accuracies,f1_scores = handler.MajorityVotingClassifier(2)\n",
    "\n",
    "print(\"Accuracy: {}, F1 Score: {}\".format(accuracies, f1_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(**params_rf)\n",
    "classifier.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(test_x)\n",
    "accuracy_score(test_y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from lime import lime_text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(lowercase=False)\n",
    "c = make_pipeline(vectorizer, classifier)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_text import LimeTextExplainer\n",
    "explainer = LimeTextExplainer(class_names=[\"NONE\", \"HOF\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors = vectorizer.fit_transform(train.post)\n",
    "test_vectors = vectorizer.transform(test.post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(c.predict_proba([test.iloc[0][\"post\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "820250bceef0ad969593e947a9fb574bdce77274dbf9ae33ffd3f4e00efdde57"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('aditi')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
