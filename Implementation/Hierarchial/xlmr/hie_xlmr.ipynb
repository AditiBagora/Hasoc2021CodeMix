{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZeea8jHSDAR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\" \n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.cuda.current_device()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mW5GaPQLq9X"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M15bUo0w_pF2"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_columns', None)  # or 1000\n",
        "pd.set_option('display.max_rows', None)  # or 1000\n",
        "pd.set_option('display.max_colwidth', None) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEYaHlngrGOD"
      },
      "outputs": [],
      "source": [
        "train = pd.read_pickle(\"train_hierarchial.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train=train.sample(1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tktq9CtaRxU"
      },
      "outputs": [],
      "source": [
        "post,context, label = train.post.values, train.context, train.label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from transformers import BertModel\n",
        "\n",
        "# from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "# bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# def bert_representations(sentence):\n",
        "#     input = tokenizer(sentence, return_tensors = \"pt\")\n",
        "#     outputs = bert(**input)\n",
        "#     return outputs.last_hidden_state[:,0,:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "class XLMR:\n",
        "    def __init__(self):\n",
        "        self.model_name = 'xlmr'\n",
        "        self.nlp = pipeline(task =\"feature-extraction\", model = 'xlm-roberta-base', tokenizer='xlm-roberta-base', framework='pt', device=0)\n",
        "\n",
        "    def GetFeatures(self, sentences=None):\n",
        "        if self.model_name == 'xlmr':\n",
        "            features = self.nlp(sentences, truncation=True) \n",
        "            # featurelist=list()\n",
        "            # for i in features:\n",
        "            #    featurelist.append(i[0][0])\n",
        "            # self.features=pd.DataFrame(featurelist)        \n",
        "        return features[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "xlmr = XLMR()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrDBKDToXT3O"
      },
      "outputs": [],
      "source": [
        "class CustomBERTModel(nn.Module):\n",
        "    def __init__(self):\n",
        "          super(CustomBERTModel, self).__init__()\n",
        "     \n",
        "          self.linear1 = nn.Linear(1536, 768)\n",
        "          self.linear2 = nn.Linear(768, 300)\n",
        "          self.linear3 = nn.Linear(1068, 768)\n",
        "          self.linear4 = nn.Linear(768,2)\n",
        "          self.outputlayer = nn.Softmax(dim=1)\n",
        "\n",
        "    def concatenate(self, tensor1, tensor2):\n",
        "      return torch.cat((tensor1, tensor2), 1)\n",
        "      \n",
        "    def forward(self, post=None, context=None, text=None, only_post = False, post_context=False, post_context_text=False):\n",
        "\n",
        "      if only_post:\n",
        "        input_post_embeddings = post\n",
        "        \n",
        "        linear4_output = self.linear4(input_post_embeddings)\n",
        "\n",
        "        outputs = self.outputlayer(linear4_output)\n",
        "\n",
        "      if post_context:\n",
        "\n",
        "        input_post_embeddings = post\n",
        "\n",
        "        input_context_embeddings = context\n",
        "\n",
        "        post_context = self.concatenate(input_post_embeddings, input_context_embeddings)\n",
        "\n",
        "        linear1_output = self.linear1(post_context)\n",
        "\n",
        "        linear4_output = self.linear4(linear1_output)\n",
        "\n",
        "        outputs = self.outputlayer(linear4_output)\n",
        "\n",
        "      if post_context_text:\n",
        "\n",
        "        input_post_embeddings = post\n",
        "\n",
        "        input_context_embeddings = context\n",
        "\n",
        "        input_text_embeddings = text\n",
        "\n",
        "        context_text = self.concatenate(input_context_embeddings, input_post_embeddings)\n",
        "\n",
        "        linear1_output = self.linear1(context_text)\n",
        "\n",
        "        linear2_output = self.linear2(linear1_output)\n",
        "\n",
        "        post_context_text = self.concatenate(input_text_embeddings, linear2_output)\n",
        "\n",
        "        linear3_output = self.linear3(post_context_text)\n",
        "        print(linear3_output.size())\n",
        "        print(linear3_output)\n",
        "\n",
        "        linear4_output = self.linear4(linear3_output)\n",
        "\n",
        "        outputs = self.outputlayer(linear4_output)\n",
        "\n",
        "      return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SESSD0_Ewj54",
        "outputId": "631c15ae-27b8-4ebe-98ef-3bdc2a704bbf"
      },
      "outputs": [],
      "source": [
        "model = CustomBERTModel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aevqgxsLwcfF",
        "outputId": "c5f5731f-82f3-4261-94fb-00f2a6010126"
      },
      "outputs": [],
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-7:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "learning_rate = 1e-4\n",
        "\n",
        "epochs = 4\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMYqxSVFw0r4",
        "outputId": "cfd1e3bd-068a-4394-ec54-861612940737"
      },
      "outputs": [],
      "source": [
        "print(\"Training....\")\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    total_train_loss=0\n",
        "\n",
        "    print(\"==========Epochs:{}===========\".format(epoch)) \n",
        "\n",
        "    for post, context,text, label in zip(train[\"post\"], train[\"context\"], train[\"text\"], train[\"label\"]):\n",
        "\n",
        "      if post!=\"\" and context==\"\" and text==\"\":\n",
        "\n",
        "        input_post=xlmr.GetFeatures(post)\n",
        "        input_post = torch.FloatTensor([input_post]).to(device)\n",
        "\n",
        "        logits = model(input_post, only_post=True)\n",
        "        \n",
        "\n",
        "      if post!=\"\" and context!=\"\" and text==\"\":\n",
        "\n",
        "        input_post=xlmr.GetFeatures(post)\n",
        "        input_post = torch.FloatTensor([input_post]).to(device)\n",
        "\n",
        "        input_context=xlmr.GetFeatures(context)\n",
        "        input_context = torch.FloatTensor([input_context]).to(device)\n",
        "\n",
        "        logits = model(input_post, input_context,post_context=True)\n",
        "\n",
        "      if post!=\"\" and context!=\"\" and text!=\"\":\n",
        "\n",
        "        input_post=xlmr.GetFeatures(post)\n",
        "        input_post = torch.Tensor([input_post]).to(device)\n",
        "\n",
        "        input_context=xlmr.GetFeatures(context)\n",
        "        input_context = torch.Tensor([input_context]).to(device)\n",
        "\n",
        "        input_text=xlmr.GetFeatures(text)\n",
        "        input_text = torch.Tensor([input_text]).to(device)\n",
        "\n",
        "        logits = model(input_post, input_context,input_text, post_context_text=True)\n",
        "       \n",
        "      # print(logits)# [[0.75, 0.25]], [0], [1]\n",
        "\n",
        "      # label = 0 => [1, 0]\n",
        "\n",
        "      # one_hot = torch.zeros(1, 2, dtype=torch.long)\n",
        "\n",
        "      # one_hot[0, label] = 1\n",
        "\n",
        "      one_hot =torch.tensor([label])\n",
        "\n",
        "      one_hot = one_hot.to(device)\n",
        "        \n",
        "      loss = criterion(input=logits, target=one_hot)\n",
        "      print(loss)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      loss.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "\n",
        "      total_train_loss += loss\n",
        "      \n",
        "      torch.save(model.state_dict(), \"rev_xlmr_model_adamw.pth\")\n",
        "    print(\"Training Loss: {}\".format((total_train_loss/len(train)).item()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4eal1WVJjn5"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"rev_xlmr_model_adamw.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIO3AMCzJmqV",
        "outputId": "2d8ae893-99b1-4d59-93c6-4e3317621bce"
      },
      "outputs": [],
      "source": [
        "model = CustomBERTModel()\n",
        "model.load_state_dict(torch.load(\"rev_xlmr_model_adamw.pth\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZeFBkSkrnve"
      },
      "outputs": [],
      "source": [
        "# Testing Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eiqaw137rAlQ"
      },
      "outputs": [],
      "source": [
        "test = pd.read_pickle(\"test_hierarchial.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6GQh_0walEC",
        "outputId": "5bab1ed4-7b8b-47b5-fdfa-1d803e3e9a35"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "print(\"Testing....\")\n",
        "\n",
        "predictions = []\n",
        "labels = []\n",
        "\n",
        "model.eval()\n",
        "\n",
        "model=model.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "  for post, context, text, label in zip(test[\"post\"], test[\"context\"], test[\"text\"], test[\"label\"]):\n",
        "\n",
        "    if post!=\"\" and context==\"\" and text==\"\":\n",
        "\n",
        "      input_post=xlmr.GetFeatures(post)\n",
        "      input_post = torch.FloatTensor([input_post]).to(device)\n",
        "\n",
        "      logits = model(input_post, only_post=True)\n",
        "        \n",
        "    if post!=\"\" and context!=\"\" and text==\"\":\n",
        "\n",
        "      input_post=xlmr.GetFeatures(post)\n",
        "      input_post = torch.FloatTensor([input_post]).to(device)\n",
        "\n",
        "      input_context=xlmr.GetFeatures(context)\n",
        "      input_context = torch.FloatTensor([input_context]).to(device)\n",
        "\n",
        "      logits = model(input_post, input_context,post_context=True)\n",
        "\n",
        "    if post!=\"\" and context!=\"\" and text!=\"\":\n",
        "\n",
        "      input_post=xlmr.GetFeatures(post)\n",
        "      input_post = torch.Tensor([input_post]).to(device)\n",
        "\n",
        "      input_context=xlmr.GetFeatures(context)\n",
        "      input_context = torch.Tensor([input_context]).to(device)\n",
        "\n",
        "      input_text=xlmr.GetFeatures(text)\n",
        "      input_text = torch.Tensor([input_text]).to(device)\n",
        "\n",
        "      logits = model(input_post, input_context,input_text, post_context_text=True)\n",
        "\n",
        "    y_pred = torch.argmax(logits, dim=-1).item()\n",
        "\n",
        "    print(logits, y_pred, label)\n",
        "\n",
        "    predictions.append(y_pred)\n",
        "\n",
        "    labels.append(label)\n",
        "\n",
        "  print(\"Accuracy: {}\".format(accuracy_score(predictions, labels)))\n",
        "  print(\"F1 Score: {}\".format(f1_score(predictions, labels)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "with open('XLMR_1e-5.pkl', 'wb') as f:\n",
        "  pickle.dump(predictions, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "preds= pd.read_pickle(\"XLMR_preds_1e-4.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "labels = test.label.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "f1_score(labels, preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "V2- Hierarchial Modelling.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
