{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\" \n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iZeea8jHSDAR"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-mW5GaPQLq9X"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "M15bUo0w_pF2"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_columns', None)  # or 1000\n",
        "pd.set_option('display.max_rows', None)  # or 1000\n",
        "pd.set_option('display.max_colwidth', None) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LOXrgvjjDLnP"
      },
      "outputs": [],
      "source": [
        "df = pd.read_pickle(\"train_p2_flattened.pkl\")\n",
        "test = pd.read_pickle(\"test_p2_flattened.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Y-Gs08_reJvc"
      },
      "outputs": [],
      "source": [
        "df[\"list_context\"] = df[\"context\"].map(lambda a: a.split(\"[SEP]\"))\n",
        "test[\"list_context\"] = test[\"context\"].map(lambda a: a.split(\"[SEP]\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ZibhIOFpoguu"
      },
      "outputs": [],
      "source": [
        "df = df.drop(['context'], axis = 1)\n",
        "df = df.drop(['tweet_id'], axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "DkuGV28uDfEB"
      },
      "outputs": [],
      "source": [
        "test = test.drop(['context'], axis = 1)\n",
        "test = test.drop(['tweet_id'], axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rlLTjtuAXq69"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "import transformers\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_text as text\n",
        "from bert import bert_tokenization\n",
        "from scipy.spatial import distance\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from transformers import pipeline\n",
        "from transformers import DistilBertTokenizer, DistilBertModel, BertTokenizer, BertModel, AutoTokenizer, AutoModelForMaskedLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "PFBhSBQHq-AG"
      },
      "outputs": [],
      "source": [
        "class model:\n",
        "    def __init__(self, df, model_name, avg_pooling=False):\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "        self.tokenized_padded_text = None\n",
        "        self.attention_mask = None\n",
        "        self.textip = None\n",
        "        self.pooledOp = None\n",
        "        self.input_dfs = None\n",
        "        self.data_frame = df\n",
        "        self.feature_df = None\n",
        "        self.model_name = None\n",
        "        self.InitModel(model_name, avg_pooling)\n",
        "\n",
        "    def InitModel(self, model_name, avg_pooling):\n",
        "      \n",
        "\n",
        "        if model_name == 'distilBert':\n",
        "            model_class, tokenizer_class, pretrained_weights = (\n",
        "                DistilBertModel, DistilBertTokenizer, 'distilbert-base-uncased')\n",
        "            self.tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
        "            self.model = model_class.from_pretrained(pretrained_weights)\n",
        "            self.model_name = 'distilBert'\n",
        "\n",
        "        if model_name == 'mBert':\n",
        "            self.tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "            self.model = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "            self.max_seq_length = 512\n",
        "            self.model_name = 'mBert'\n",
        "        \n",
        "        if model_name == 'mBert_p':\n",
        "            self.model_name = 'mBert_p'\n",
        "            self.nlp = pipeline(task =\"feature-extraction\", model = 'bert-base-multilingual-cased', tokenizer='bert-base-multilingual-cased', framework='pt', device=0)\n",
        "\n",
        "\n",
        "        if model_name == 'muril':\n",
        "            self.textip = tf.keras.layers.Input(shape=(), dtype=tf.string)\n",
        "            self.max_seq_length = 128\n",
        "            muril_model, muril_layer = self.init_muril(\n",
        "                model_url=\"https://tfhub.dev/google/MuRIL/1\", max_seq_length=self.max_seq_length,\n",
        "                avg_pooling=avg_pooling)\n",
        "            vocab_file = muril_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "            do_lower_case = muril_layer.resolved_object.do_lower_case.numpy()\n",
        "            self.tokenizer = bert_tokenization.FullTokenizer(vocab_file, do_lower_case)\n",
        "            self.model_name = 'muril'\n",
        "            self.model = muril_model\n",
        "            self.avg_pooling = avg_pooling\n",
        "        if model_name=='xlmr':    \n",
        "            self.model_name = 'xlmr'\n",
        "            self.nlp = pipeline(task =\"feature-extraction\", model = 'xlm-roberta-base', tokenizer='xlm-roberta-base', framework='pt', device=0)\n",
        "            self.avg_pooling = avg_pooling \n",
        "            \n",
        "    def tokenize(self, column):\n",
        "        tokenized_text = column.apply((lambda x: self.tokenizer.encode(x,truncation=True,add_special_tokens=True)))\n",
        "        max_len = 0\n",
        "        for i in tokenized_text.values:\n",
        "            if len(i) > max_len:\n",
        "                max_len = len(i)\n",
        "        self.tokenized_padded_text = np.array([i + [0]*(max_len-len(i)) for i in tokenized_text.values])\n",
        "        self.create_attention_mask()\n",
        "\n",
        "    def create_attention_mask(self):\n",
        "        self.attention_mask = np.where(self.tokenized_padded_text != 0, 1, 0)\n",
        "        print(type(self.tokenized_padded_text))\n",
        "        self.input_ids = torch.tensor(self.tokenized_padded_text)\n",
        "        self.attention_mask = torch.tensor(self.attention_mask)\n",
        "\n",
        "    def init_muril(self, model_url, max_seq_length, avg_pooling):\n",
        "        inputs = dict(\n",
        "            input_word_ids=tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32),\n",
        "            input_mask=tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32),\n",
        "            input_type_ids=tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32),\n",
        "        )\n",
        "\n",
        "        muril_layer = hub.KerasLayer(model_url, trainable=True)\n",
        "        outputs = muril_layer(inputs)\n",
        "        print(outputs)\n",
        "        assert 'sequence_output' in outputs\n",
        "        assert 'pooled_output' in outputs\n",
        "        assert 'encoder_outputs' in outputs\n",
        "        assert 'default' in outputs\n",
        "        if avg_pooling:\n",
        "            return tf.keras.Model(inputs=inputs, outputs=outputs[\"encoder_outputs\"]), muril_layer\n",
        "        else:\n",
        "            return tf.keras.Model(inputs=inputs, outputs=outputs[\"pooled_output\"]), muril_layer\n",
        "\n",
        "    def create_input(self, input_strings, tokenizer, max_seq_length):\n",
        "        input_ids_all, input_mask_all, input_type_ids_all = [], [], []\n",
        "        for input_string in input_strings:\n",
        "            input_tokens = [\"[CLS]\"] + tokenizer.tokenize(input_string) + [\"[SEP]\"]\n",
        "            input_ids = tokenizer.convert_tokens_to_ids(input_tokens)\n",
        "            sequence_length = min(len(input_ids), max_seq_length)\n",
        "\n",
        "            if len(input_ids) >= max_seq_length:\n",
        "                input_ids = input_ids[:max_seq_length]\n",
        "            else:\n",
        "                input_ids = input_ids + [0] * (max_seq_length - len(input_ids))\n",
        "\n",
        "            input_mask = [1] * sequence_length + [0] * (max_seq_length - sequence_length)\n",
        "\n",
        "            input_ids_all.append(input_ids)\n",
        "            input_mask_all.append(input_mask)\n",
        "            input_type_ids_all.append([0] * max_seq_length)\n",
        "\n",
        "        return np.array(input_ids_all), np.array(input_mask_all), np.array(input_type_ids_all)\n",
        "\n",
        "    def encode(self, input_text):\n",
        "        input_ids, input_mask, input_type_ids = self.create_input(input_text,\n",
        "                                                                  self.tokenizer,\n",
        "                                                                  self.max_seq_length)\n",
        "        inputs = dict(\n",
        "            input_word_ids=input_ids,\n",
        "            input_mask=input_mask,\n",
        "            input_type_ids=input_type_ids,\n",
        "        )\n",
        "        return self.model(inputs)\n",
        "\n",
        "    def GetFeatures(self, input=None):\n",
        "        if self.model_name == 'mBert':\n",
        "            with torch.no_grad():\n",
        "                last_hidden_states = self.model(self.input_ids, attention_mask=self.attention_mask)\n",
        "            last_hidden_states['last_hidden_state'].size()\n",
        "            self.features = last_hidden_states[0][:, 0, :].numpy()\n",
        "            self.features = pd.DataFrame(self.features)\n",
        "        elif self.model_name == 'muril':\n",
        "            embeddings = self.encode(input)\n",
        "            if not self.avg_pooling:\n",
        "                self.features = pd.DataFrame(embeddings.numpy())\n",
        "            else:\n",
        "                f1 = embeddings[7][:, 0, :].numpy()\n",
        "                f2 = embeddings[6][:, 0, :].numpy()\n",
        "                f3 = embeddings[5][:, 0, :].numpy()\n",
        "                self.features = pd.DataFrame((f1+f2+f3)/3)\n",
        "        elif self.model_name == 'xlmr':\n",
        "            sentences=input\n",
        "            features = self.nlp(sentences, truncation=True) \n",
        "            featurelist=list()\n",
        "            for i in features:\n",
        "               featurelist.append(i[0][0])\n",
        "            self.features=pd.DataFrame(featurelist)     \n",
        "        elif self.model_name == 'mBert_p':\n",
        "            sentences=input\n",
        "            features = self.nlp(sentences, truncation=True) \n",
        "            featurelist=list()\n",
        "            for i in features:\n",
        "               featurelist.append(i[0][0])\n",
        "            self.features=pd.DataFrame(featurelist)           \n",
        "        return self.features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "class classifiers:\n",
        "\n",
        "  def __init__(self,features_train,label_train,features_test,label_test):\n",
        "    self.train_features=features_train\n",
        "    self.train_labels=label_train\n",
        "    self.test_features=features_test\n",
        "    self.test_labels=label_test\n",
        "    self.accuracy=list()\n",
        "    self.f1score=list()\n",
        "    self.models=list()\n",
        "    self.y_pred=list()\n",
        "\n",
        "  def classify(self,svm=True,random_forest=True,xgboost=True,logistic_regression=True,ann=True)  :\n",
        "      if svm==True:\n",
        "        acc,f1_score=self.CreateSVMClassifier()\n",
        "        self.accuracy.append(acc)\n",
        "        self.f1score.append(f1_score)\n",
        "        \n",
        "        self.models.append('svm')\n",
        "      if random_forest==True:\n",
        "        acc,f1_score,y=self.RandomForestClassifier()\n",
        "        self.accuracy.append(acc)\n",
        "        self.f1score.append(f1_score)\n",
        "        self.y_pred.append(y)\n",
        "        self.models.append('random_forest')\n",
        "      if xgboost==True:\n",
        "        acc,f1_score,y=self.XGBClassifier(2)\n",
        "        self.accuracy.append(acc)\n",
        "        self.f1score.append(f1_score)\n",
        "        self.y_pred.append(y)\n",
        "        self.models.append('xgboost')\n",
        "      if logistic_regression==True:\n",
        "        acc,f1_score,y=self.LogisticRegression()\n",
        "        self.accuracy.append(acc)\n",
        "        self.f1score.append(f1_score)\n",
        "        self.y_pred.append(y)\n",
        "        self.models.append('lr')\n",
        "      if ann==True:\n",
        "        acc,f1_score=self.annClassifier()\n",
        "        self.accuracy.append(acc)\n",
        "        self.f1score.append(f1_score)\n",
        "        self.models.append('ann')\n",
        "      return self.accuracy,self.f1score,self.models,self.y_pred\n",
        "\n",
        "  def MajorityVotingClassifier(self, num_class):\n",
        "        acc_xg, f1_xg, y_xgboost = self.XGBClassifier(num_class)\n",
        "        acc_rf, f1_rf, y_rf = self.RandomForestClassifier()\n",
        "        acc_rf, f1_rf, y_lr = self.LogisticRegression()\n",
        "        y_pred = list()\n",
        "        for i in range(len(y_xgboost)):\n",
        "            preds = list()\n",
        "            preds.append(y_xgboost[i])\n",
        "            preds.append(y_rf[i])\n",
        "            preds.append(y_lr[i])\n",
        "            y_pred.append(max(set(preds), key=preds.count))\n",
        "        cm=confusion_matrix(self.test_labels,y_pred)    \n",
        "        return accuracy_score(self.test_labels, y_pred), f1_score(self.test_labels, y_pred, average='macro') ,cm,y_pred \n",
        "\n",
        "  def VotingClassifier(self):\n",
        "      clf1 = LogisticRegression(multi_class='multinomial', random_state=1)\n",
        "      clf2 = RandomForestClassifier(n_estimators=50, random_state=1)   \n",
        "      clf3=XGBClassifier(n_estimators=500,learning_rate=1, max_depth=2,objective='multi:softmax',num_class=2) \n",
        "      eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('xgb', clf3)], voting='hard') \n",
        "      eclf.fit(self.train_features,self.train_labels)\n",
        "      y_pred= eclf.predict(self.test_features) \n",
        "      return print(accuracy_score(self.test_labels, y_pred)),print(f1_score(self.test_labels, y_pred,average='macro')),y_pred\n",
        "\n",
        "  def XGBClassifier(self,num_class):\n",
        "     from xgboost import XGBClassifier\n",
        "     classifier = XGBClassifier(n_estimators=500,learning_rate=1, max_depth=2,objective='multi:softmax',num_class=num_class)\n",
        "     classifier.fit(self.train_features, self.train_labels)\n",
        "     \n",
        "     y_pred = classifier.predict(self.test_features)\n",
        "     return print(accuracy_score(self.test_labels, y_pred)),print(f1_score(self.test_labels, y_pred,average='macro')),y_pred\n",
        "\n",
        "  def CreateSVMClassifier(self):\n",
        "    from sklearn.svm import SVC\n",
        "    classifier = SVC(kernel = 'poly',decision_function_shape='ovr', random_state = 0)\n",
        "    classifier.fit(self.train_features, self.train_labels)\n",
        "  \n",
        "    y_pred = classifier.predict(self.test_features)\n",
        "    return accuracy_score(self.test_labels, y_pred),f1_score(self.test_labels, y_pred,average='macro')\n",
        "\n",
        "  def RandomForestClassifier(self):\n",
        "     from sklearn.ensemble import RandomForestClassifier\n",
        "     classifier = RandomForestClassifier(n_estimators = 500, criterion = 'entropy', random_state = 0)\n",
        "     classifier.fit(self.train_features, self.train_labels)\n",
        "\n",
        "     y_pred = classifier.predict(self.test_features)\n",
        "     return print(accuracy_score(self.test_labels, y_pred)),print(f1_score(self.test_labels, y_pred,average='macro')),y_pred\n",
        "\n",
        "  def Createstaticsplit(self,features,labels,split_per=0.8):\n",
        "   num=np.shape(features)[0]\n",
        "   self.train_features=features.head(int(split_per*num))\n",
        "   self.train_labels=labels.head(int(split_per*num))\n",
        "   self.test_features=features.tail(num-int(split_per*num))\n",
        "   self.test_labels=labels.tail(num-int(split_per*num))\n",
        "  def annClassifier(self):\n",
        "      import tensorflow as tf\n",
        "      from sklearn.compose import ColumnTransformer\n",
        "      from sklearn.preprocessing import OneHotEncoder\n",
        "      ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [1])], remainder='passthrough')\n",
        "      feature_set = np.array(ct.fit_transform(self.features_set))\n",
        "      train_features, test_features, train_labels, test_labels = train_test_split(feature_set, self.labels)\n",
        "\n",
        "      ann = tf.keras.models.Sequential()\n",
        "      ann.add(tf.keras.layers.Dense(units=6, activation='relu'))\n",
        "      ann.add(tf.keras.layers.Dense(units=6, activation='relu'))\n",
        "      ann.add(tf.keras.layers.Dense(units=1, activation='softmax'))\n",
        "      ann.add(tf.keras.layers.Dense(units=1, activation='softmax'))\n",
        "      ann.add(tf.keras.layers.Dense(units=1, activation='softmax'))\n",
        "      ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "      ann.fit(train_features, train_labels, batch_size = 32, epochs = 200)\n",
        "\n",
        "      y_pred = ann.predict(test_features)\n",
        "      return accuracy_score(test_labels, y_pred),f1_score(test_labels, y_pred,average='macro')\n",
        "\n",
        "  def LogisticRegression(self):\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.linear_model import LogisticRegression \n",
        "    lr_clf = LogisticRegression(multi_class='multinomial')\n",
        "    lr_clf.fit(self.train_features, self.train_labels) \n",
        "\n",
        "    y_pred=lr_clf.predict(self.test_features)\n",
        "    return print(accuracy_score(self.test_labels, y_pred)),print(f1_score(self.test_labels, y_pred,average='macro')),y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def classify(train_dataframe,test_dataframe,classifier):\n",
        "   classifier_class=classifiers(features_train=train_dataframe.iloc[:, :-1],label_train=train_dataframe.label,features_test=test_dataframe.iloc[:, :-1],label_test=test_dataframe.label)\n",
        "   test_labels=test_dataframe.label\n",
        "   if classifier == 'A':\n",
        "    accuracies,f1_scores,models,y=classifier_class.classify(svm=True,random_forest=True,xgboost=True,logistic_regression=True,ann=False)\n",
        "    for i in range(len(y)) :\n",
        "      print(models[i])\n",
        "      print(classification_report(test_labels, y[i], labels=[0,1]))\n",
        "    a,f,cm,y_pred=classifier_class.MajorityVotingClassifier(2)  \n",
        "    print(\"VC\")\n",
        "    print(classification_report(test_labels, y_pred, labels=[0,1]))   \n",
        "   elif classifier == 'LR':\n",
        "      accuracies,f1_scores,models,y=classifier_class.classify(svm=False,random_forest=False,xgboost=False,logistic_regression=True,ann=False)\n",
        "      print(classification_report(test_labels, y[0], labels=[0,1]))\n",
        "   elif classifier == 'RF':  \n",
        "      accuracies,f1_scores,models,y=classifier_class.classify(svm=False,random_forest=True,xgboost=False,logistic_regression=False,ann=False)\n",
        "      print(classification_report(test_labels, y[0], labels=[0,1]))\n",
        "   elif classifier == 'XGBOOST':\n",
        "      accuracies,f1_scores,models,y=classifier_class.classify(svm=False,random_forest=False,xgboost=True,logistic_regression=False,ann=False)\n",
        "      print(classification_report(test_labels, y[0], labels=[0,1]))\n",
        "   elif classifier == 'M_VC': \n",
        "      a,f,cm,y_pred=classifier_class.MajorityVotingClassifier(2)  \n",
        "      print(classification_report(test_labels, y_pred, labels=[0,1])) \n",
        "   elif classifier == 'VC': \n",
        "      a,f,y_pred=classifier_class.VotingClassifier()  \n",
        "      print(classification_report(test_labels, y_pred, labels=[0,1])) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4v4M9V0vq-Cl",
        "outputId": "61e8073d-56e5-48a2-a183-31f40648fb25"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:absl:Using /tmp/tfhub_modules to cache modules.\n",
            "2022-01-11 19:36:30.624927: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
            "2022-01-11 19:36:30.625084: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
            "2022-01-11 19:36:30.628634: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
            "2022-01-11 19:36:30.628750: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
            "2022-01-11 19:36:30.628854: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
            "2022-01-11 19:36:30.628872: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n",
            "2022-01-11 19:36:30.629414: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'sequence_output': <KerasTensor: shape=(None, 128, 768) dtype=float32 (created by layer 'keras_layer')>, 'default': <KerasTensor: shape=(None, 768) dtype=float32 (created by layer 'keras_layer')>, 'pooled_output': <KerasTensor: shape=(None, 768) dtype=float32 (created by layer 'keras_layer')>, 'encoder_outputs': [<KerasTensor: shape=(None, 128, 768) dtype=float32 (created by layer 'keras_layer')>, <KerasTensor: shape=(None, 128, 768) dtype=float32 (created by layer 'keras_layer')>, <KerasTensor: shape=(None, 128, 768) dtype=float32 (created by layer 'keras_layer')>, <KerasTensor: shape=(None, 128, 768) dtype=float32 (created by layer 'keras_layer')>, <KerasTensor: shape=(None, 128, 768) dtype=float32 (created by layer 'keras_layer')>, <KerasTensor: shape=(None, 128, 768) dtype=float32 (created by layer 'keras_layer')>, <KerasTensor: shape=(None, 128, 768) dtype=float32 (created by layer 'keras_layer')>, <KerasTensor: shape=(None, 128, 768) dtype=float32 (created by layer 'keras_layer')>, <KerasTensor: shape=(None, 128, 768) dtype=float32 (created by layer 'keras_layer')>, <KerasTensor: shape=(None, 128, 768) dtype=float32 (created by layer 'keras_layer')>, <KerasTensor: shape=(None, 128, 768) dtype=float32 (created by layer 'keras_layer')>, <KerasTensor: shape=(None, 128, 768) dtype=float32 (created by layer 'keras_layer')>]}\n"
          ]
        }
      ],
      "source": [
        "model_pipeline=model(df,model_name='muril')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Zbgnv3A5-_HN"
      },
      "outputs": [],
      "source": [
        "def cosine_similarity(list1, list2, distance=False):\n",
        "  if distance:\n",
        "    return spatial.distance.cosine(list1, list2)\n",
        "\n",
        "  return 1 - spatial.distance.cosine(list1, list2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "sDOQoeO1RU4t"
      },
      "outputs": [],
      "source": [
        "from operator import add\n",
        "from scipy import spatial\n",
        "\n",
        "\n",
        "def get_final_embeddings(df):\n",
        "\n",
        "  row_embeddings = []\n",
        "\n",
        "  for text, context in zip(df[\"text\"], df[\"list_context\"]):\n",
        "    \n",
        "    text_embeddings = model_pipeline.GetFeatures([text]).iloc[0].to_list()\n",
        "\n",
        "    final_embeddings = np.zeros(768)\n",
        "\n",
        "    final_embeddings = list( map(add, text_embeddings, final_embeddings) )\n",
        "\n",
        "    for sub_context in context:\n",
        "\n",
        "      context_embeddings = model_pipeline.GetFeatures([sub_context]).iloc[0].to_list()\n",
        "\n",
        "      cosine_distance = cosine_similarity(text_embeddings, context_embeddings, distance=True)\n",
        "\n",
        "      weighted_contexts = [x * cosine_distance for x in context_embeddings]\n",
        "\n",
        "      final_embeddings = list( map(add, weighted_contexts, final_embeddings))\n",
        "\n",
        "    row_embeddings.append(final_embeddings)\n",
        "\n",
        "  return row_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkhUEXQNELNF"
      },
      "outputs": [],
      "source": [
        "df[\"final_embeddings\"] = get_final_embeddings(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rd5oR60RD3pt"
      },
      "outputs": [],
      "source": [
        "test[\"final_embeddings\"] = get_final_embeddings(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a84O5CVbLQnl"
      },
      "outputs": [],
      "source": [
        "TASK1 = {\"HOF\": 1, \"NONE\": 0}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mmq6I3VlLFxi"
      },
      "outputs": [],
      "source": [
        "df['label']=df['label'].map(TASK1)\n",
        "test['label']=test['label'].map(TASK1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6NGtxeF8Tm7"
      },
      "outputs": [],
      "source": [
        "train_X = pd.DataFrame(df['final_embeddings'].tolist())\n",
        "train_y = df[\"label\"]\n",
        "train_X['label']=df['label']\n",
        "train_X.to_pickle('muril_p2_train.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6nv_pAFFRGU"
      },
      "outputs": [],
      "source": [
        "test_X = pd.DataFrame(test['final_embeddings'].tolist())\n",
        "test_y = test[\"label\"]\n",
        "test_X['label']=test['label']\n",
        "test_X.to_pickle('muril_p2_test.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_X=pd.read_pickle('muril_p2_train.pkl')\n",
        "test_X=pd.read_pickle('muril_p2_test.pkl')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "CHqiuJ_uHFsm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.5999043977055449\n",
            "0.5998779756294024\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.60      0.60      1055\n",
            "           1       0.60      0.60      0.60      1037\n",
            "\n",
            "    accuracy                           0.60      2092\n",
            "   macro avg       0.60      0.60      0.60      2092\n",
            "weighted avg       0.60      0.60      0.60      2092\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Classification Head\n",
        "classify(train_X,test_X,'RF')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/raid/cs18resch11003/anaconda3/envs/aditi/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[19:25:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softmax' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "0.5927342256214149\n",
            "0.5785337838093472\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.77      0.66      1055\n",
            "           1       0.64      0.41      0.50      1037\n",
            "\n",
            "    accuracy                           0.59      2092\n",
            "   macro avg       0.60      0.59      0.58      2092\n",
            "weighted avg       0.60      0.59      0.58      2092\n",
            "\n"
          ]
        }
      ],
      "source": [
        "classify(train_X,test_X,'VC')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.504302103250478\n",
            "0.3352399110263743\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      1.00      0.67      1055\n",
            "           1       0.00      0.00      0.00      1037\n",
            "\n",
            "    accuracy                           0.50      2092\n",
            "   macro avg       0.25      0.50      0.34      2092\n",
            "weighted avg       0.25      0.50      0.34      2092\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/raid/cs18resch11003/anaconda3/envs/aditi/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/raid/cs18resch11003/anaconda3/envs/aditi/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/raid/cs18resch11003/anaconda3/envs/aditi/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "classify(train_X,test_X,'LR')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/raid/cs18resch11003/anaconda3/envs/aditi/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[19:28:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softmax' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "0.6123326959847036\n",
            "0.6123177253818584\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.61      0.61      1055\n",
            "           1       0.61      0.61      0.61      1037\n",
            "\n",
            "    accuracy                           0.61      2092\n",
            "   macro avg       0.61      0.61      0.61      2092\n",
            "weighted avg       0.61      0.61      0.61      2092\n",
            "\n"
          ]
        }
      ],
      "source": [
        "classify(train_X,test_X,'XGBOOST')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/raid/cs18resch11003/anaconda3/envs/aditi/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[19:30:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softmax' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "0.6123326959847036\n",
            "0.6123177253818584\n",
            "0.5999043977055449\n",
            "0.5998779756294024\n",
            "0.504302103250478\n",
            "0.3352399110263743\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.75      0.66      1055\n",
            "           1       0.65      0.47      0.55      1037\n",
            "\n",
            "    accuracy                           0.61      2092\n",
            "   macro avg       0.62      0.61      0.61      2092\n",
            "weighted avg       0.62      0.61      0.61      2092\n",
            "\n"
          ]
        }
      ],
      "source": [
        "classify(train_X,test_X,'M_VC')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0AVNTGJ1MTND"
      },
      "outputs": [],
      "source": [
        "params_xgboost = {\"n_estimators\": 500, \"learning_rate\": 1, \"max_depth\": 2,\n",
        "                  \"objective\": 'multi:softmax', \"num_class\": 4}\n",
        "params_lr = {\"multi_class\": 'multinomial'}\n",
        "params_rf = {\"n_estimators\": 500, \"criterion\": 'entropy', \"random_state\": 0}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "soKt8djfIVRJ"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "class Classifiers:\n",
        "    def __init__(self, train_X, train_y, test_X, test_y):\n",
        "        self.Createstaticsplit(train_X, train_y, test_X, test_y)\n",
        "        self.accuracy = list()\n",
        "        self.f1score = list()\n",
        "\n",
        "    def MajorityVotingClassifier(self, num_class):\n",
        "        acc_xg, f1_xg, y_xgboost = self.XGBClassifier(num_class)\n",
        "        acc_rf, f1_rf, y_rf = self.RandomForestClassifier()\n",
        "        acc_rf, f1_rf, y_lr = self.LogisticRegression()\n",
        "        y_pred = list()\n",
        "        for i in range(len(y_xgboost)):\n",
        "            preds = list()\n",
        "            preds.append(y_xgboost[i])\n",
        "            preds.append(y_rf[i])\n",
        "            preds.append(y_lr[i])\n",
        "            y_pred.append(max(set(preds), key=preds.count))\n",
        "\n",
        "        with open('p2_predictions.pkl', 'wb') as f:\n",
        "          pickle.dump(y_pred, f)\n",
        "\n",
        "        return accuracy_score(\n",
        "            self.test_labels, y_pred), f1_score(\n",
        "            self.test_labels, y_pred, average='weighted')\n",
        "\n",
        "    def XGBClassifier(self, num_class):\n",
        "        classifier = XGBClassifier(**params_xgboost)\n",
        "        classifier.fit(self.train_features, self.train_labels)\n",
        "        y_pred = classifier.predict(self.test_features)\n",
        "        return accuracy_score(self.test_labels, y_pred), f1_score(self.test_labels, y_pred, average='weighted'), y_pred\n",
        "\n",
        "    def RandomForestClassifier(self):\n",
        "        classifier = RandomForestClassifier(**params_rf)\n",
        "        classifier.fit(self.train_features, self.train_labels)\n",
        "        y_pred = classifier.predict(self.test_features)\n",
        "        return accuracy_score(self.test_labels, y_pred), f1_score(self.test_labels, y_pred, average='weighted'), y_pred\n",
        "\n",
        "    def Createstaticsplit(self, train_X, train_y, test_X, test_y):\n",
        "        self.train_features = train_X\n",
        "        self.train_labels = train_y\n",
        "        self.test_features = test_X\n",
        "        self.test_labels = test_y\n",
        "\n",
        "    def LogisticRegression(self):\n",
        "        lr_clf = LogisticRegression(**params_lr)\n",
        "        lr_clf.fit(self.train_features, self.train_labels)\n",
        "        y_pred = lr_clf.predict(self.test_features)\n",
        "        return accuracy_score(self.test_labels, y_pred), f1_score(self.test_labels, y_pred, average='weighted'), y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P00N_M1cjJyI",
        "outputId": "b70d9e0d-0796-4be5-e68f-8de7e4ecd140"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/raid/cs18resch11003/anaconda3/envs/aditi/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[09:56:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softmax' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "Accuracy: 0.6500956022944551, F1 Score: 0.644136907034282\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/raid/cs18resch11003/anaconda3/envs/aditi/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "handler = Classifiers(train_X, train_y, test_X, test_y)\n",
        "\n",
        "accuracies,f1_scores = handler.MajorityVotingClassifier(2)\n",
        "\n",
        "print(\"Accuracy: {}, F1 Score: {}\".format(accuracies, f1_scores))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Propose2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
