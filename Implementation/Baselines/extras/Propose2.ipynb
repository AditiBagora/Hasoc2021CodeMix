{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":50269,"status":"ok","timestamp":1641385873474,"user":{"displayName":"Kamal Shrestha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRmbrhanuWLNu07Db5etQmkaTAbYTjqIM3oCin=s64","userId":"06925189110392645592"},"user_tz":-345},"id":"mXI2rqCVKng2","outputId":"560d7070-1e10-4403-eb72-771c16284013"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n","/content/drive/MyDrive/HASOC Project Folder/2021 Dataset/CodeMix Dataset\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","% cd /content/drive/MyDrive/HASOC Project Folder/2021 Dataset/CodeMix Dataset/"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"iZeea8jHSDAR"},"outputs":[],"source":["import warnings\n","warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"-mW5GaPQLq9X"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","#import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"M15bUo0w_pF2"},"outputs":[],"source":["pd.set_option('display.max_columns', None)  # or 1000\n","pd.set_option('display.max_rows', None)  # or 1000\n","pd.set_option('display.max_colwidth', None) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1YHWlIN6Lym_"},"outputs":[],"source":["from glob import glob\n","import re\n","import json\n","\n","train_directories = []\n","for i in glob(\"data/train/*/\"):\n","    for j in glob(i+'*/'):\n","        train_directories.append(j)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ga_oclqnMJ23"},"outputs":[],"source":["data = []\n","for i in train_directories:\n","    with open(i+'data.json', encoding='utf-8') as f:\n","        data.append(json.load(f))\n","labels = []\n","for i in train_directories:\n","    with open(i+'labels.json', encoding='utf-8') as f:\n","        labels.append(json.load(f))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-ZeSyR8ZAQfG"},"outputs":[],"source":["def p2_flatten(d, l):\n","\n","    flat_text = []\n","\n","    flat_text.append({\n","        'tweet_id': d['tweet_id'],\n","        \"context\": d['tweet'],\n","        'text': d['tweet'],\n","        'label': l[d['tweet_id']]\n","    })\n","\n","    for i in d['comments']:\n","        previous_tweet = \"\"\n","\n","        flat_text.append({\n","            'tweet_id': i['tweet_id'],\n","            'context': d[\"tweet\"],\n","            'text': i[\"tweet\"],\n","            'label': l[i['tweet_id']]\n","        })\n","\n","        if 'replies' in i.keys():\n","            for j in i['replies']:\n","                flat_text.append({\n","                    'tweet_id': j['tweet_id'],\n","                    'context': d[\"tweet\"] + i[\"tweet\"] + previous_tweet,\n","                    'text': j['tweet'],\n","                    'label': l[j['tweet_id']]\n","                })\n","                previous_tweet = previous_tweet + \" [SEP] \"+ j[\"tweet\"]\n","\n","    return flat_text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MxNXAaKYAQfH"},"outputs":[],"source":["data_label = []\n","\n","for i in range(len(labels)):\n","    for j in p2_flatten(data[i], labels[i]):\n","        data_label.append(j)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vZ1hBU79AQfI"},"outputs":[],"source":["df = pd.DataFrame(data_label, columns = data_label[0].keys(), index = None)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4wRY4oz9C3yf"},"outputs":[],"source":["df.to_pickle(\"p2_flattened.pkl\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fhFPNNuaAPzI"},"outputs":[],"source":["df = df.head(25)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["train_df=pd.read_pickle('p2_codemix_flat.pkl')\n","test_df=pd.read_pickle('p2_codemix_flat_test.pkl')"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"Y-Gs08_reJvc"},"outputs":[],"source":["train_df[\"list_context\"] = train_df[\"context\"].map(lambda a: a.split(\" [SEP] \"))\n","test_df[\"list_context\"] = test_df[\"context\"].map(lambda a: a.split(\" [SEP] \"))"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["test_df = test_df.drop(['context'], axis = 1)\n","test_df = test_df.drop(['tweet_id'], axis = 1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_df"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":31,"status":"ok","timestamp":1641387717011,"user":{"displayName":"Kamal Shrestha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRmbrhanuWLNu07Db5etQmkaTAbYTjqIM3oCin=s64","userId":"06925189110392645592"},"user_tz":-345},"id":"ZibhIOFpoguu","outputId":"542cc310-af11-4aba-fec3-89bc51f42024"},"outputs":[],"source":["train_df = train_df.drop(['context'], axis = 1)\n","train_df = train_df.drop(['tweet_id'], axis = 1)\n"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":678,"status":"ok","timestamp":1641387721381,"user":{"displayName":"Kamal Shrestha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRmbrhanuWLNu07Db5etQmkaTAbYTjqIM3oCin=s64","userId":"06925189110392645592"},"user_tz":-345},"id":"XqTHW-FzAP9N","outputId":"45a6621a-b414-4b0c-a6a0-f0eca40324f9"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to\n","[nltk_data]     /home/cs18resch11003/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to\n","[nltk_data]     /home/cs18resch11003/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"BRA6kF8IAP_z"},"outputs":[],"source":["from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","\n","def similarity_score(X,Y, distance=False):\n","  # tokenization\n","  X_list = word_tokenize(X) \n","  Y_list = word_tokenize(Y)\n","    \n","  # sw contains the list of stopwords\n","  sw = stopwords.words('english') \n","  l1 =[];l2 =[]\n","    \n","  # remove stop words from the string\n","  X_set = {w for w in X_list if not w in sw} \n","  Y_set = {w for w in Y_list if not w in sw}\n","    \n","  # form a set containing keywords of both strings \n","  rvector = X_set.union(Y_set) \n","  for w in rvector:\n","      if w in X_set: l1.append(1) # create a vector\n","      else: l1.append(0)\n","      if w in Y_set: l2.append(1)\n","      else: l2.append(0)\n","  c = 0\n","  # cosine formula \n","  for i in range(len(rvector)):\n","          c+= l1[i]*l2[i]\n","  cosine = c / float((sum(l1)*sum(l2))**0.5)\n","  \n","  if distance:\n","    return c\n","  return cosine"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"JHyHoxRsAP1c"},"outputs":[],"source":["sims_df = []\n","for index in range(len(train_df)):\n","  sims = []\n","  normal_sims = []\n","  sum_sims = 0\n","\n","  text = train_df.iloc[index][\"text\"]\n","\n","  for context in train_df.iloc[index][\"list_context\"]:\n","\n","    similarity = similarity_score(text, context, distance=True)\n","\n","    if similarity==0:\n","      similarity = 0.001\n","\n","    sum_sims = sum_sims+similarity\n","    \n","    sims.append(similarity)\n","\n","  for i, sim in enumerate(sims):\n","    normal_sims.append(sims[i]/sum_sims)\n","\n","  sims_df.append(sims)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"amAb3IrPXquz"},"outputs":[],"source":["train_df[\"weights\"] = sims_df"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1641387726669,"user":{"displayName":"Kamal Shrestha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRmbrhanuWLNu07Db5etQmkaTAbYTjqIM3oCin=s64","userId":"06925189110392645592"},"user_tz":-345},"id":"FGVPylU1Xqz9","outputId":"c30612a9-2b5b-438f-de80-b41f8128eef1"},"outputs":[],"source":["train_df"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["sims_df = []\n","for index in range(len(test_df)):\n","  sims = []\n","  normal_sims = []\n","  sum_sims = 0\n","\n","  text = test_df.iloc[index][\"text\"]\n","\n","  for context in test_df.iloc[index][\"list_context\"]:\n","\n","    similarity = similarity_score(text, context, distance=True)\n","\n","    if similarity==0:\n","      similarity = 0.001\n","\n","    sum_sims = sum_sims+similarity\n","    \n","    sims.append(similarity)\n","\n","  for i, sim in enumerate(sims):\n","    normal_sims.append(sims[i]/sum_sims)\n","\n","  sims_df.append(sims)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["test_df[\"weights\"] = sims_df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B76Puk2_Xq4y"},"outputs":[],"source":["!pip install transformers --quiet\n","!pip install bert-for-tf2 --quiet\n","!pip install tensorflow-text --quiet"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"rlLTjtuAXq69"},"outputs":[],"source":["import numpy as np\n","import torch\n","import pandas as pd\n","import transformers\n","from transformers import pipeline\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","import tensorflow_datasets as tfds\n","import tensorflow_text as text\n","from bert import bert_tokenization\n","from scipy.spatial import distance\n","from sklearn.metrics import classification_report\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n","from xgboost import XGBClassifier\n","from transformers import DistilBertTokenizer, DistilBertModel, BertTokenizer, BertModel, AutoTokenizer, AutoModelForMaskedLM"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["class model:\n","    def __init__(self, df, model_name, avg_pooling=False):\n","        self.tokenizer = None\n","        self.model = None\n","        self.tokenized_padded_text = None\n","        self.attention_mask = None\n","        self.textip = None\n","        self.pooledOp = None\n","        self.input_dfs = None\n","        self.data_frame = df\n","        self.feature_df = None\n","        self.model_name = None\n","        self.InitModel(model_name, avg_pooling)\n","\n","    def InitModel(self, model_name, avg_pooling):\n","      \n","\n","        if model_name == 'distilBert':\n","            model_class, tokenizer_class, pretrained_weights = (\n","                DistilBertModel, DistilBertTokenizer, 'distilbert-base-uncased')\n","            self.tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n","            self.model = model_class.from_pretrained(pretrained_weights)\n","            self.model_name = 'distilBert'\n","\n","        if model_name == 'mBert':\n","            self.tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n","            self.model = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n","            self.max_seq_length = 512\n","            self.model_name = 'mBert'\n","        \n","        if model_name == 'mBert_p':\n","            #self.tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n","            #self.model = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n","            #self.max_seq_length = 512\n","            self.model_name = 'mBert_p'\n","            self.nlp = pipeline(task =\"feature-extraction\", model = 'bert-base-multilingual-cased', tokenizer='bert-base-multilingual-cased', framework='pt', device=0)\n","\n","\n","        if model_name == 'muril':\n","            self.textip = tf.keras.layers.Input(shape=(), dtype=tf.string)\n","            self.max_seq_length = 128\n","            muril_model, muril_layer = self.init_muril(\n","                model_url=\"https://tfhub.dev/google/MuRIL/1\", max_seq_length=self.max_seq_length,\n","                avg_pooling=avg_pooling)\n","            vocab_file = muril_layer.resolved_object.vocab_file.asset_path.numpy()\n","            do_lower_case = muril_layer.resolved_object.do_lower_case.numpy()\n","            self.tokenizer = bert_tokenization.FullTokenizer(vocab_file, do_lower_case)\n","            self.model_name = 'muril'\n","            self.model = muril_model\n","            self.avg_pooling = avg_pooling\n","        if model_name=='xlmr':    \n","            self.model_name = 'xlmr'\n","            self.nlp = pipeline(task =\"feature-extraction\", model = 'xlm-roberta-base', tokenizer='xlm-roberta-base', framework='pt', device=0)\n","            self.avg_pooling = avg_pooling \n","            \n","    def tokenize(self, column):\n","        tokenized_text = column.apply((lambda x: self.tokenizer.encode(x,truncation=True,add_special_tokens=True)))\n","        max_len = 0\n","        for i in tokenized_text.values:\n","            if len(i) > max_len:\n","                max_len = len(i)\n","        self.tokenized_padded_text = np.array([i + [0]*(max_len-len(i)) for i in tokenized_text.values])\n","        self.create_attention_mask()\n","\n","    def create_attention_mask(self):\n","        self.attention_mask = np.where(self.tokenized_padded_text != 0, 1, 0)\n","        print(type(self.tokenized_padded_text))\n","        self.input_ids = torch.tensor(self.tokenized_padded_text)\n","        self.attention_mask = torch.tensor(self.attention_mask)\n","\n","    def init_muril(self, model_url, max_seq_length, avg_pooling):\n","        inputs = dict(\n","            input_word_ids=tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32),\n","            input_mask=tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32),\n","            input_type_ids=tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32),\n","        )\n","\n","        muril_layer = hub.KerasLayer(model_url, trainable=True)\n","        outputs = muril_layer(inputs)\n","        print(outputs)\n","        assert 'sequence_output' in outputs\n","        assert 'pooled_output' in outputs\n","        assert 'encoder_outputs' in outputs\n","        assert 'default' in outputs\n","        if avg_pooling:\n","            return tf.keras.Model(inputs=inputs, outputs=outputs[\"encoder_outputs\"]), muril_layer\n","        else:\n","            return tf.keras.Model(inputs=inputs, outputs=outputs[\"pooled_output\"]), muril_layer\n","\n","    def create_input(self, input_strings, tokenizer, max_seq_length):\n","        input_ids_all, input_mask_all, input_type_ids_all = [], [], []\n","        for input_string in input_strings:\n","            input_tokens = [\"[CLS]\"] + tokenizer.tokenize(input_string) + [\"[SEP]\"]\n","            input_ids = tokenizer.convert_tokens_to_ids(input_tokens)\n","            sequence_length = min(len(input_ids), max_seq_length)\n","\n","            if len(input_ids) >= max_seq_length:\n","                input_ids = input_ids[:max_seq_length]\n","            else:\n","                input_ids = input_ids + [0] * (max_seq_length - len(input_ids))\n","\n","            input_mask = [1] * sequence_length + [0] * (max_seq_length - sequence_length)\n","\n","            input_ids_all.append(input_ids)\n","            input_mask_all.append(input_mask)\n","            input_type_ids_all.append([0] * max_seq_length)\n","\n","        return np.array(input_ids_all), np.array(input_mask_all), np.array(input_type_ids_all)\n","\n","    def encode(self, input_text):\n","        input_ids, input_mask, input_type_ids = self.create_input(input_text,\n","                                                                  self.tokenizer,\n","                                                                  self.max_seq_length)\n","        inputs = dict(\n","            input_word_ids=input_ids,\n","            input_mask=input_mask,\n","            input_type_ids=input_type_ids,\n","        )\n","        return self.model(inputs)\n","\n","    def GetFeatures(self, input=None):\n","        if self.model_name == 'mBert':\n","            with torch.no_grad():\n","                last_hidden_states = self.model(self.input_ids, attention_mask=self.attention_mask)\n","            last_hidden_states['last_hidden_state'].size()\n","            self.features = last_hidden_states[0][:, 0, :].numpy()\n","            self.features = pd.DataFrame(self.features)\n","        elif self.model_name == 'muril':\n","            embeddings = self.encode(input)\n","            if not self.avg_pooling:\n","                self.features = pd.DataFrame(embeddings.numpy())\n","            else:\n","                f1 = embeddings[7][:, 0, :].numpy()\n","                f2 = embeddings[6][:, 0, :].numpy()\n","                f3 = embeddings[5][:, 0, :].numpy()\n","                self.features = pd.DataFrame((f1+f2+f3)/3)\n","        elif self.model_name == 'xlmr':\n","            sentences=input\n","            features = self.nlp(sentences, truncation=True) \n","            featurelist=list()\n","            for i in features:\n","               featurelist.append(i[0][0])\n","            self.features=pd.DataFrame(featurelist)     \n","        elif self.model_name == 'mBert_p':\n","            sentences=input\n","            features = self.nlp(sentences, truncation=True) \n","            featurelist=list()\n","            for i in features:\n","               featurelist.append(i[0][0])\n","            self.features=pd.DataFrame(featurelist)           \n","        return self.features"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["class classifiers:\n","\n","  def __init__(self,features_train,label_train,features_test,label_test):\n","    #self.features_set=features\n","    #self.labels=label\n","    #self.Createstaticsplit(features,label)\n","    self.train_features=features_train\n","    self.train_labels=label_train\n","    self.test_features=features_test\n","    self.test_labels=label_test\n","    self.accuracy=list()\n","    self.f1score=list()\n","    self.models=list()\n","    self.y_pred=list()\n","\n","  def classify(self,svm=True,random_forest=True,xgboost=True,logistic_regression=True,ann=True)  :\n","      if svm==True:\n","        acc,f1_score=self.CreateSVMClassifier()\n","        self.accuracy.append(acc)\n","        self.f1score.append(f1_score)\n","        \n","        self.models.append('svm')\n","      if random_forest==True:\n","        acc,f1_score,y=self.RandomForestClassifier()\n","        self.accuracy.append(acc)\n","        self.f1score.append(f1_score)\n","        self.y_pred.append(y)\n","        self.models.append('random_forest')\n","      if xgboost==True:\n","        acc,f1_score,y=self.XGBClassifier(2)\n","        self.accuracy.append(acc)\n","        self.f1score.append(f1_score)\n","        self.y_pred.append(y)\n","        self.models.append('xgboost')\n","      if logistic_regression==True:\n","        acc,f1_score,y=self.LogisticRegression()\n","        self.accuracy.append(acc)\n","        self.f1score.append(f1_score)\n","        self.y_pred.append(y)\n","        self.models.append('lr')\n","      if ann==True:\n","        acc,f1_score=self.annClassifier()\n","        self.accuracy.append(acc)\n","        self.f1score.append(f1_score)\n","        self.models.append('ann')\n","      return self.accuracy,self.f1score,self.models,self.y_pred\n","\n","  def MajorityVotingClassifier(self, num_class):\n","        acc_xg, f1_xg, y_xgboost = self.XGBClassifier(num_class)\n","        acc_rf, f1_rf, y_rf = self.RandomForestClassifier()\n","        acc_rf, f1_rf, y_lr = self.LogisticRegression()\n","        y_pred = list()\n","        for i in range(len(y_xgboost)):\n","            preds = list()\n","            preds.append(y_xgboost[i])\n","            preds.append(y_rf[i])\n","            preds.append(y_lr[i])\n","            y_pred.append(max(set(preds), key=preds.count))\n","        cm=confusion_matrix(self.test_labels,y_pred)    \n","        return accuracy_score(self.test_labels, y_pred), f1_score(self.test_labels, y_pred, average='macro') ,cm,y_pred \n","\n","  def VotingClassifier(self):\n","      clf1 = LogisticRegression(multi_class='multinomial', random_state=1)\n","      clf2 = RandomForestClassifier(n_estimators=50, random_state=1)   \n","      clf3=XGBClassifier(n_estimators=500,learning_rate=1, max_depth=2,objective='multi:softmax',num_class=2) \n","      eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('xgb', clf3)], voting='hard') \n","      eclf.fit(self.train_features,self.train_labels)\n","      y_pred= eclf.predict(self.test_features) \n","      return print(accuracy_score(self.test_labels, y_pred)),print(f1_score(self.test_labels, y_pred,average='macro')),y_pred\n","\n","  def XGBClassifier(self,num_class):\n","     from xgboost import XGBClassifier\n","     classifier = XGBClassifier(n_estimators=500,learning_rate=1, max_depth=2,objective='multi:softmax',num_class=num_class)\n","     classifier.fit(self.train_features, self.train_labels)\n","     \n","     y_pred = classifier.predict(self.test_features)\n","     return print(accuracy_score(self.test_labels, y_pred)),print(f1_score(self.test_labels, y_pred,average='macro')),y_pred\n","\n","  def CreateSVMClassifier(self):\n","    from sklearn.svm import SVC\n","    classifier = SVC(kernel = 'poly',decision_function_shape='ovr', random_state = 0)\n","    classifier.fit(self.train_features, self.train_labels)\n","  \n","    y_pred = classifier.predict(self.test_features)\n","    return accuracy_score(self.test_labels, y_pred),f1_score(self.test_labels, y_pred,average='macro')\n","\n","  def RandomForestClassifier(self):\n","     from sklearn.ensemble import RandomForestClassifier\n","     classifier = RandomForestClassifier(n_estimators = 500, criterion = 'entropy', random_state = 0)\n","     classifier.fit(self.train_features, self.train_labels)\n","\n","     y_pred = classifier.predict(self.test_features)\n","     return print(accuracy_score(self.test_labels, y_pred)),print(f1_score(self.test_labels, y_pred,average='macro')),y_pred\n","\n","  def Createstaticsplit(self,features,labels,split_per=0.8):\n","   num=np.shape(features)[0]\n","   self.train_features=features.head(int(split_per*num))\n","   self.train_labels=labels.head(int(split_per*num))\n","   self.test_features=features.tail(num-int(split_per*num))\n","   self.test_labels=labels.tail(num-int(split_per*num))\n","  def annClassifier(self):\n","      import tensorflow as tf\n","      from sklearn.compose import ColumnTransformer\n","      from sklearn.preprocessing import OneHotEncoder\n","      ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [1])], remainder='passthrough')\n","      feature_set = np.array(ct.fit_transform(self.features_set))\n","      #self.labels.replace('NOT',0,inplace=True)\n","      #self.labels.replace('HOF',1,inplace=True)\n","      train_features, test_features, train_labels, test_labels = train_test_split(feature_set, self.labels)\n","\n","      ann = tf.keras.models.Sequential()\n","      ann.add(tf.keras.layers.Dense(units=6, activation='relu'))\n","      ann.add(tf.keras.layers.Dense(units=6, activation='relu'))\n","      ann.add(tf.keras.layers.Dense(units=1, activation='softmax'))\n","      ann.add(tf.keras.layers.Dense(units=1, activation='softmax'))\n","      ann.add(tf.keras.layers.Dense(units=1, activation='softmax'))\n","      ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n","      ann.fit(train_features, train_labels, batch_size = 32, epochs = 200)\n","\n","      y_pred = ann.predict(test_features)\n","     # y_pred = (y_pred > 0.5)\n","      return accuracy_score(test_labels, y_pred),f1_score(test_labels, y_pred,average='macro')\n","\n","  def LogisticRegression(self):\n","    from sklearn.model_selection import train_test_split\n","    from sklearn.linear_model import LogisticRegression \n","    lr_clf = LogisticRegression(multi_class='multinomial')\n","    lr_clf.fit(self.train_features, self.train_labels) \n","\n","    y_pred=lr_clf.predict(self.test_features)\n","    return print(accuracy_score(self.test_labels, y_pred)),print(f1_score(self.test_labels, y_pred,average='macro')),y_pred"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["def classify(train_dataframe,test_dataframe,classifier):\n","   classifier_class=classifiers(features_train=train_dataframe.iloc[:, :-1],label_train=train_dataframe.label,features_test=test_dataframe.iloc[:, :-1],label_test=test_dataframe.label)\n","   test_labels=test_dataframe.label\n","   if classifier == 'A':\n","    accuracies,f1_scores,models,y=classifier_class.classify(svm=True,random_forest=True,xgboost=True,logistic_regression=True,ann=False)\n","    for i in range(len(y)) :\n","      print(models[i])\n","      print(classification_report(test_labels, y[i], labels=[0,1]))\n","    a,f,cm,y_pred=classifier_class.MajorityVotingClassifier(2)  \n","    print(\"VC\")\n","    print(classification_report(test_labels, y_pred, labels=[0,1]))   \n","   elif classifier == 'LR':\n","      accuracies,f1_scores,models,y=classifier_class.classify(svm=False,random_forest=False,xgboost=False,logistic_regression=True,ann=False)\n","      print(classification_report(test_labels, y[0], labels=[0,1]))\n","   elif classifier == 'RF':  \n","      accuracies,f1_scores,models,y=classifier_class.classify(svm=False,random_forest=True,xgboost=False,logistic_regression=False,ann=False)\n","      print(classification_report(test_labels, y[0], labels=[0,1]))\n","   elif classifier == 'XGBOOST':\n","      accuracies,f1_scores,models,y=classifier_class.classify(svm=False,random_forest=False,xgboost=True,logistic_regression=False,ann=False)\n","      print(classification_report(test_labels, y[0], labels=[0,1]))\n","   elif classifier == 'M_VC': \n","      a,f,cm,y_pred=classifier_class.MajorityVotingClassifier(2)  \n","      print(classification_report(test_labels, y_pred, labels=[0,1])) \n","   elif classifier == 'VC': \n","      a,f,y_pred=classifier_class.VotingClassifier()  \n","      print(classification_report(test_labels, y_pred, labels=[0,1])) "]},{"cell_type":"code","execution_count":19,"metadata":{"id":"4v4M9V0vq-Cl"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["model_pipeline=model(train_df,model_name='mBert_p')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1bVst4BASMGd"},"outputs":[],"source":["df.sample(2)"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"sDOQoeO1RU4t"},"outputs":[{"name":"stderr","output_type":"stream","text":["/raid/cs18resch11003/anaconda3/envs/aditi/lib/python3.8/site-packages/transformers/pipelines/base.py:1075: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n"]}],"source":["from operator import add\n","\n","row_embeddings = []\n","for weight,context in zip(train_df[\"list_context\"], train_df[\"weights\"]):\n","  final_embeddings = np.zeros(768)\n","  for sub_weight, sub_context in zip(context, weight):\n","    text_embeddings = model_pipeline.GetFeatures(sub_context).iloc[0].to_list()\n","    text_embeddings = list(map(lambda x:sub_weight*x, text_embeddings))\n","    final_embeddings  = list( map(add, final_embeddings, text_embeddings) )\n","\n","  row_embeddings.append(final_embeddings)"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"ijQfwhK9AP4E"},"outputs":[],"source":["train_df[\"context_embeddings\"] = row_embeddings"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"r--3PhITYeSk"},"outputs":[],"source":["row_embeddings = []\n","for text in train_df[\"text\"]:\n","  text_embeddings = model_pipeline.GetFeatures(text).iloc[0].to_list()\n","  row_embeddings.append(text_embeddings)"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"SfdkUgJfYeSm"},"outputs":[],"source":["train_df[\"text_embeddings\"] = row_embeddings"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"uiF50HmbAQEx"},"outputs":[],"source":["from operator import add\n","\n","row_embeddings = []\n","for context,text in zip(train_df[\"context_embeddings\"], train_df[\"text_embeddings\"]):\n","  final_embeddings = np.zeros(768)\n","\n","  final_embeddings  = list(map(add, context, text) )\n","\n","  row_embeddings.append(final_embeddings)\n","\n","train_df[\"training_embeddings\"] = row_embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0kVOLOI8aITO"},"outputs":[],"source":["train_df.sample(5)"]},{"cell_type":"code","execution_count":69,"metadata":{"id":"8ElkPCDG7dSE"},"outputs":[{"data":{"text/plain":["array([1, 0])"]},"execution_count":69,"metadata":{},"output_type":"execute_result"}],"source":["train_df[\"label\"].unique()"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"a84O5CVbLQnl"},"outputs":[],"source":["TASK1 = {\"HOF\": 1, \"NONE\": 0}"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"Mmq6I3VlLFxi"},"outputs":[],"source":["train_df['label']=train_df['label'].map(TASK1)"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"v6NGtxeF8Tm7"},"outputs":[],"source":["train_X = pd.DataFrame(train_df['training_embeddings'].tolist())"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"l8Q4SmI6-Lq8"},"outputs":[],"source":["train_y = train_df[\"label\"]"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["model_pipeline=model(test_df,model_name='mBert_p')"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["from operator import add\n","\n","row_embeddings = []\n","for weight,context in zip(test_df[\"list_context\"], test_df[\"weights\"]):\n","  final_embeddings = np.zeros(768)\n","  for sub_weight, sub_context in zip(context, weight):\n","    text_embeddings = model_pipeline.GetFeatures(sub_context).iloc[0].to_list()\n","    text_embeddings = list(map(lambda x:sub_weight*x, text_embeddings))\n","    final_embeddings  = list( map(add, final_embeddings, text_embeddings) )\n","\n","  row_embeddings.append(final_embeddings)\n","test_df[\"context_embeddings\"] = row_embeddings  "]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["row_embeddings = []\n","for text in test_df[\"text\"]:\n","  text_embeddings = model_pipeline.GetFeatures(text).iloc[0].to_list()\n","  row_embeddings.append(text_embeddings)\n","test_df[\"text_embeddings\"] = row_embeddings  "]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["from operator import add\n","row_embeddings = []\n","for context,text in zip(test_df[\"context_embeddings\"], test_df[\"text_embeddings\"]):\n","  final_embeddings = np.zeros(768)\n","  final_embeddings  = list(map(add, context, text) )\n","  row_embeddings.append(final_embeddings)\n","test_df[\"training_embeddings\"] = row_embeddings"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["test_df['label']=test_df['label'].map(TASK1)"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[],"source":["test_X = pd.DataFrame(test_df['training_embeddings'].tolist())"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[],"source":["test_y = test_df[\"label\"]"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[],"source":["train_X['label']=train_y\n","test_X['label']=test_y"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[],"source":["train_X.to_pickle('mbert_p2_train.pkl')\n","test_X.to_pickle('mbert_p2_test.pkl')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_df['label']"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.5066344993968637\n","0.5043728667382931\n","              precision    recall  f1-score   support\n","\n","           0       0.41      0.54      0.47       334\n","           1       0.61      0.48      0.54       495\n","\n","    accuracy                           0.51       829\n","   macro avg       0.51      0.51      0.50       829\n","weighted avg       0.53      0.51      0.51       829\n","\n"]}],"source":["classify(train_X,test_X,'RF')"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/raid/cs18resch11003/anaconda3/envs/aditi/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[16:12:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softmax' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","0.4957780458383595\n","0.4948776704916503\n","0.5066344993968637\n","0.5043728667382931\n","0.4077201447527141\n","0.3009385760779027\n","              precision    recall  f1-score   support\n","\n","           0       0.41      0.64      0.50       334\n","           1       0.61      0.38      0.46       495\n","\n","    accuracy                           0.48       829\n","   macro avg       0.51      0.51      0.48       829\n","weighted avg       0.53      0.48      0.48       829\n","\n"]}],"source":["classify(train_X,test_X,'VC')"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.4077201447527141\n","0.3009385760779027\n","              precision    recall  f1-score   support\n","\n","           0       0.40      0.99      0.57       334\n","           1       0.70      0.01      0.03       495\n","\n","    accuracy                           0.41       829\n","   macro avg       0.55      0.50      0.30       829\n","weighted avg       0.58      0.41      0.25       829\n","\n"]}],"source":["classify(train_X,test_X,'LR')"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/raid/cs18resch11003/anaconda3/envs/aditi/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[16:14:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softmax' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","0.4957780458383595\n","0.4948776704916503\n","              precision    recall  f1-score   support\n","\n","           0       0.41      0.56      0.47       334\n","           1       0.60      0.45      0.52       495\n","\n","    accuracy                           0.50       829\n","   macro avg       0.51      0.51      0.49       829\n","weighted avg       0.53      0.50      0.50       829\n","\n"]}],"source":["classify(train_X,test_X,'XGBOOST')"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["train_df=pd.read_pickle('mbert_p1_train.pkl')\n","test_df=pd.read_pickle('mbert_p1_test.pkl')"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/raid/cs18resch11003/anaconda3/envs/aditi/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n","/raid/cs18resch11003/anaconda3/envs/aditi/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[22:37:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softmax' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","0.5971049457177322\n","0.5778610545317056\n","              precision    recall  f1-score   support\n","\n","           0       0.66      0.68      0.67       495\n","           1       0.50      0.48      0.49       334\n","\n","    accuracy                           0.60       829\n","   macro avg       0.58      0.58      0.58       829\n","weighted avg       0.59      0.60      0.60       829\n","\n"]}],"source":["classify(train_df,test_df,'VC')"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[],"source":["train_df=pd.read_pickle('muril_p1_train.pkl')\n","test_df=pd.read_pickle('muril_p1_test.pkl')"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/raid/cs18resch11003/anaconda3/envs/aditi/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[22:39:10] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softmax' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","0.51145958986731\n","0.5111152853192614\n","              precision    recall  f1-score   support\n","\n","           0       0.63      0.45      0.52       495\n","           1       0.42      0.60      0.50       334\n","\n","    accuracy                           0.51       829\n","   macro avg       0.53      0.53      0.51       829\n","weighted avg       0.55      0.51      0.51       829\n","\n"]}],"source":["classify(train_df,test_df,'VC')"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["train_df=pd.read_pickle('muril_p2_train.pkl')\n","test_df=pd.read_pickle('muril_p2_test.pkl')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/raid/cs18resch11003/anaconda3/envs/aditi/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[17:38:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softmax' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]}],"source":["classify(train_df,test_df,'VC')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["classify(train_df,test_df,'RF')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["classify(train_df,test_df,'M_VC')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["classify(train_df,test_df,'LR')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["classify(train_df,test_df,'XGBOOST')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0AVNTGJ1MTND"},"outputs":[],"source":["params_xgboost = {\"n_estimators\": 500, \"learning_rate\": 1, \"max_depth\": 2,\n","                  \"objective\": 'multi:softmax', \"num_class\": 4}\n","params_lr = {\"multi_class\": 'multinomial'}\n","params_rf = {\"n_estimators\": 500, \"criterion\": 'entropy', \"random_state\": 0}\n","TRAIN_RATIO = 0.8"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"soKt8djfIVRJ"},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression\n","from xgboost import XGBClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n","\n","class Classifiers:\n","    def __init__(self, features, label):\n","        self.features_set = features\n","        self.labels = label\n","        self.Createstaticsplit(features, label)\n","        self.accuracy = list()\n","        self.f1score = list()\n","\n","    def MajorityVotingClassifier(self, num_class):\n","        acc_xg, f1_xg, y_xgboost = self.XGBClassifier(num_class)\n","        acc_rf, f1_rf, y_rf = self.RandomForestClassifier()\n","        acc_rf, f1_rf, y_lr = self.LogisticRegression()\n","        y_pred = list()\n","        for i in range(len(y_xgboost)):\n","            preds = list()\n","            preds.append(y_xgboost[i])\n","            preds.append(y_rf[i])\n","            preds.append(y_lr[i])\n","            y_pred.append(max(set(preds), key=preds.count))\n","        return accuracy_score(\n","            self.test_labels, y_pred), f1_score(\n","            self.test_labels, y_pred, average='weighted')\n","\n","    def XGBClassifier(self, num_class):\n","        classifier = XGBClassifier(**params_xgboost)\n","        classifier.fit(self.train_features, self.train_labels)\n","        y_pred = classifier.predict(self.test_features)\n","        return accuracy_score(self.test_labels, y_pred), f1_score(self.test_labels, y_pred, average='weighted'), y_pred\n","\n","    def RandomForestClassifier(self):\n","        classifier = RandomForestClassifier(**params_rf)\n","        classifier.fit(self.train_features, self.train_labels)\n","        y_pred = classifier.predict(self.test_features)\n","        return accuracy_score(self.test_labels, y_pred), f1_score(self.test_labels, y_pred, average='weighted'), y_pred\n","\n","    def Createstaticsplit(self, features, labels, split_per=0.8):\n","        num = np.shape(features)[0]\n","        self.train_features = features.head(int(split_per*num))\n","        self.train_labels = labels.head(int(split_per*num))\n","        self.test_features = features.tail(num-int(split_per*num))\n","        self.test_labels = labels.tail(num-int(split_per*num))\n","\n","    def LogisticRegression(self):\n","        lr_clf = LogisticRegression(**params_lr)\n","        lr_clf.fit(self.train_features, self.train_labels)\n","        y_pred = lr_clf.predict(self.test_features)\n","        return accuracy_score(self.test_labels, y_pred), f1_score(self.test_labels, y_pred, average='weighted'), y_pred"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P00N_M1cjJyI"},"outputs":[],"source":["handler=Classifiers(features=X,label=y)\n","\n","accuracies,f1_scores=handler.MajorityVotingClassifier(2)\n","\n","print(\"Accuracy: {}, F1 Score: {}\".format(accuracies, f1_scores))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vjbKUh1O_An2"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNxNYyFsLRKHKWcIp3Vd/Ke","collapsed_sections":[],"name":"Propose2.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"}},"nbformat":4,"nbformat_minor":0}
