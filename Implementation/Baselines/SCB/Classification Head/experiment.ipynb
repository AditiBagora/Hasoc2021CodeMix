{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDp9Qj1oaDjV"
      },
      "outputs": [],
      "source": [
        "!pip install transformers --quiet\n",
        "!pip install bert-for-tf2 --quiet\n",
        "!pip install tensorflow-text --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yoEIueZvv762"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qb95gDqG1X77"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\"\n",
        "import numpy as np\n",
        "import glob\n",
        "import torch\n",
        "import sys\n",
        "import pandas as pd\n",
        "import transformers\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_text as text\n",
        "from bert import bert_tokenization\n",
        "from scipy.spatial import distance\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
        "from transformers import DistilBertTokenizer, DistilBertModel, BertTokenizer, BertModel, AutoTokenizer, AutoModelForMaskedLM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gp_SqxsD63uZ"
      },
      "outputs": [],
      "source": [
        "class model:\n",
        "  def __init__(self,df,model_name):\n",
        "    self.tokenizer=None\n",
        "    self.model=None\n",
        "    self.tokenized_padded_text=None\n",
        "    self.attention_mask=None\n",
        "    self.textip=None\n",
        "    self.pooledOp=None\n",
        "    self.input_dfs=None\n",
        "    self.data_frame=df\n",
        "    self.feature_df=None\n",
        "    self.model_name=None\n",
        "    self.InitModel(model_name)\n",
        "\n",
        "  def InitModel(self,model_name) :\n",
        "    from transformers import DistilBertTokenizer, DistilBertModel, BertTokenizer, BertModel\n",
        "    if model_name == 'distilBert':\n",
        "      model_class, tokenizer_class, pretrained_weights = (DistilBertModel, DistilBertTokenizer, 'distilbert-base-uncased')  \n",
        "      self.tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
        "      self.model = model_class.from_pretrained(pretrained_weights)\n",
        "      self.model_name='distilBert'\n",
        "    if model_name == 'mBert':\n",
        "      self.tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "      self.model = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "      self.model_name='mBert'\n",
        "    if model_name=='muril':\n",
        "      self.textip = tf.keras.layers.Input(shape=(), dtype=tf.string)\n",
        "      self.max_seq_length = 128\n",
        "      muril_model, muril_layer =self.init_muril(model_url=\"https://tfhub.dev/google/MuRIL/1\", max_seq_length=self.max_seq_length)\n",
        "      vocab_file = muril_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "      do_lower_case = muril_layer.resolved_object.do_lower_case.numpy()\n",
        "      self.tokenizer = bert_tokenization.FullTokenizer(vocab_file, do_lower_case)\n",
        "      self.model_name='muril'\n",
        "      self.model=muril_model\n",
        "  \n",
        "  def tokenize(self,column):\n",
        "       tokenized_text=column.apply((lambda x: self.tokenizer.encode(x, add_special_tokens=True)))\n",
        "       # Padding\n",
        "       max_len = 0\n",
        "       for i in tokenized_text.values:\n",
        "        if len(i) > max_len:\n",
        "         max_len = len(i)\n",
        "\n",
        "       self.tokenized_padded_text = np.array([i + [0]*(max_len-len(i)) for i in tokenized_text.values])\n",
        "       self.create_attention_mask()\n",
        "\n",
        "  def create_attention_mask(self):\n",
        "      self.attention_mask = np.where(self.tokenized_padded_text != 0, 1, 0)\n",
        "      self.input_ids = torch.tensor(self.tokenized_padded_text)  \n",
        "      self.attention_mask = torch.tensor(self.attention_mask)\n",
        "\n",
        "  def init_muril(self,model_url, max_seq_length):\n",
        "    inputs = dict(\n",
        "    input_word_ids=tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32),\n",
        "    input_mask=tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32),\n",
        "    input_type_ids=tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32),\n",
        "    )\n",
        "\n",
        "    muril_layer = hub.KerasLayer(model_url, trainable=True)\n",
        "    outputs = muril_layer(inputs)\n",
        "\n",
        "    assert 'sequence_output' in outputs\n",
        "    assert 'pooled_output' in outputs\n",
        "    assert 'encoder_outputs' in outputs\n",
        "    assert 'default' in outputs\n",
        "    return tf.keras.Model(inputs=inputs,outputs=outputs[\"encoder_outputs\"]), muril_layer\n",
        "  \n",
        "  def create_input(self,input_strings, tokenizer, max_seq_length):\n",
        "     input_ids_all, input_mask_all, input_type_ids_all = [], [], []\n",
        "     for input_string in input_strings:\n",
        "       input_tokens = [\"[CLS]\"] + tokenizer.tokenize(input_string) + [\"[SEP]\"]\n",
        "       input_ids = tokenizer.convert_tokens_to_ids(input_tokens)\n",
        "       sequence_length = min(len(input_ids), max_seq_length)\n",
        "    \n",
        "       if len(input_ids) >= max_seq_length:\n",
        "        input_ids = input_ids[:max_seq_length]\n",
        "       else:\n",
        "        input_ids = input_ids + [0] * (max_seq_length - len(input_ids))\n",
        "\n",
        "       input_mask = [1] * sequence_length + [0] * (max_seq_length - sequence_length)\n",
        "\n",
        "       input_ids_all.append(input_ids)\n",
        "       input_mask_all.append(input_mask)\n",
        "       input_type_ids_all.append([0] * max_seq_length)\n",
        "  \n",
        "     return np.array(input_ids_all), np.array(input_mask_all), np.array(input_type_ids_all)\n",
        "\n",
        "  def encode(self,input_text):\n",
        "      input_ids, input_mask, input_type_ids = self.create_input(input_text, \n",
        "                                                       self.tokenizer, \n",
        "                                                       self.max_seq_length)\n",
        "      inputs = dict(\n",
        "       input_word_ids=input_ids,\n",
        "       input_mask=input_mask,\n",
        "       input_type_ids=input_type_ids,\n",
        "      )\n",
        "      return self.model(inputs)\n",
        "\n",
        "  def GetFeatures(self,input=None):\n",
        "   if self.model_name!='muril':\n",
        "      with torch.no_grad():\n",
        "       last_hidden_states = self.model(self.input_ids, attention_mask=self.attention_mask)\n",
        "      last_hidden_states['last_hidden_state'].size()   \n",
        "      self.features = last_hidden_states[0][:,0,:].numpy()\n",
        "      self.features=pd.DataFrame(self.features)\n",
        "   elif self.model_name=='muril':\n",
        "      embeddings = self.encode(input)\n",
        "      f=embeddings[11][:,0,:]\n",
        "      self.features=pd.DataFrame(f.numpy())\n",
        "   return self.features           \n",
        "         "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "soKt8djfIVRJ"
      },
      "outputs": [],
      "source": [
        "class classifiers:\n",
        "\n",
        "  def __init__(self,features,label):\n",
        "    self.features_set=features\n",
        "    self.labels=label\n",
        "    self.Createstaticsplit(features,label)\n",
        "    self.accuracy=list()\n",
        "    self.f1score=list()\n",
        "    self.models=list()\n",
        "    self.y_pred=list()\n",
        "\n",
        "  def classify(self,svm=True,random_forest=True,xgboost=True,logistic_regression=True,ann=True)  :\n",
        "      if svm==True:\n",
        "        acc,f1_score=self.CreateSVMClassifier()\n",
        "        self.accuracy.append(acc)\n",
        "        self.f1score.append(f1_score)\n",
        "        \n",
        "        self.models.append('svm')\n",
        "      if random_forest==True:\n",
        "        acc,f1_score,y=self.RandomForestClassifier()\n",
        "        self.accuracy.append(acc)\n",
        "        self.f1score.append(f1_score)\n",
        "        self.y_pred.append(y)\n",
        "        self.models.append('random_forest')\n",
        "      if xgboost==True:\n",
        "        acc,f1_score,y_pred=self.XGBClassifier(2)\n",
        "        self.accuracy.append(acc)\n",
        "        self.f1score.append(f1_score)\n",
        "        self.y_pred.append(y)\n",
        "        self.models.append('xgboost')\n",
        "      if logistic_regression==True:\n",
        "        acc,f1_score,y=self.LogisticRegression()\n",
        "        self.accuracy.append(acc)\n",
        "        self.f1score.append(f1_score)\n",
        "        self.y_pred.append(y)\n",
        "        self.models.append('lr')\n",
        "      if ann==True:\n",
        "        acc,f1_score=self.annClassifier()\n",
        "        self.accuracy.append(acc)\n",
        "        self.f1score.append(f1_score)\n",
        "        self.models.append('ann')\n",
        "      return self.accuracy,self.f1score,self.models,self.y_pred\n",
        "\n",
        "  def MajorityVotingClassifier(self, num_class):\n",
        "        acc_xg, f1_xg, y_xgboost = self.XGBClassifier(num_class)\n",
        "        acc_rf, f1_rf, y_rf = self.RandomForestClassifier()\n",
        "        acc_rf, f1_rf, y_lr = self.LogisticRegression()\n",
        "        y_pred = list()\n",
        "        for i in range(len(y_xgboost)):\n",
        "            preds = list()\n",
        "            preds.append(y_xgboost[i])\n",
        "            preds.append(y_rf[i])\n",
        "            preds.append(y_lr[i])\n",
        "            y_pred.append(max(set(preds), key=preds.count))\n",
        "        cm=confusion_matrix(self.test_labels,y_pred)    \n",
        "        return accuracy_score(self.test_labels, y_pred), f1_score(self.test_labels, y_pred, average='macro') ,cm,y_pred \n",
        "\n",
        "  def XGBClassifier(self,num_class):\n",
        "     from xgboost import XGBClassifier\n",
        "     classifier = XGBClassifier(n_estimators=500,learning_rate=1, max_depth=2,objective='multi:softmax',num_class=num_class)\n",
        "     classifier.fit(self.train_features, self.train_labels)\n",
        "     \n",
        "     y_pred = classifier.predict(self.test_features)\n",
        "     return accuracy_score(self.test_labels, y_pred),f1_score(self.test_labels, y_pred,average='macro'),y_pred\n",
        "\n",
        "  def CreateSVMClassifier(self):\n",
        "    from sklearn.svm import SVC\n",
        "    classifier = SVC(kernel = 'poly',decision_function_shape='ovr', random_state = 0)\n",
        "    classifier.fit(self.train_features, self.train_labels)\n",
        "  \n",
        "    y_pred = classifier.predict(self.test_features)\n",
        "    return accuracy_score(self.test_labels, y_pred),f1_score(self.test_labels, y_pred,average='macro')\n",
        "\n",
        "  def RandomForestClassifier(self):\n",
        "     from sklearn.ensemble import RandomForestClassifier\n",
        "     classifier = RandomForestClassifier(n_estimators = 500, criterion = 'entropy', random_state = 0)\n",
        "     classifier.fit(self.train_features, self.train_labels)\n",
        "\n",
        "     y_pred = classifier.predict(self.test_features)\n",
        "     return accuracy_score(self.test_labels, y_pred),f1_score(self.test_labels, y_pred,average='macro'),y_pred\n",
        "\n",
        "  def Createstaticsplit(self,features,labels,split_per=0.8):\n",
        "   num=np.shape(features)[0]\n",
        "   self.train_features=features.head(int(split_per*num))\n",
        "   self.train_labels=labels.head(int(split_per*num))\n",
        "   self.test_features=features.tail(num-int(split_per*num))\n",
        "   self.test_labels=labels.tail(num-int(split_per*num))\n",
        "  def annClassifier(self):\n",
        "      import tensorflow as tf\n",
        "      from sklearn.compose import ColumnTransformer\n",
        "      from sklearn.preprocessing import OneHotEncoder\n",
        "      ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [1])], remainder='passthrough')\n",
        "      feature_set = np.array(ct.fit_transform(self.features_set))\n",
        "      train_features, test_features, train_labels, test_labels = train_test_split(feature_set, self.labels)\n",
        "\n",
        "      ann = tf.keras.models.Sequential()\n",
        "      ann.add(tf.keras.layers.Dense(units=6, activation='relu'))\n",
        "      ann.add(tf.keras.layers.Dense(units=6, activation='relu'))\n",
        "      ann.add(tf.keras.layers.Dense(units=1, activation='softmax'))\n",
        "      ann.add(tf.keras.layers.Dense(units=1, activation='softmax'))\n",
        "      ann.add(tf.keras.layers.Dense(units=1, activation='softmax'))\n",
        "      ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "      ann.fit(train_features, train_labels, batch_size = 32, epochs = 200)\n",
        "\n",
        "      y_pred = ann.predict(test_features)\n",
        "\n",
        "      return accuracy_score(test_labels, y_pred),f1_score(test_labels, y_pred,average='macro')\n",
        "\n",
        "  def LogisticRegression(self):\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.linear_model import LogisticRegression \n",
        "    lr_clf = LogisticRegression(multi_class='multinomial')\n",
        "    lr_clf.fit(self.train_features, self.train_labels) \n",
        "\n",
        "    y_pred=lr_clf.predict(self.test_features)\n",
        "    return accuracy_score(self.test_labels, y_pred),f1_score(self.test_labels, y_pred,average='macro'),y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LU3xfikYp7I2",
        "outputId": "fcc78fe0-8419-4194-9de9-a3c89454db04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current Device : cuda\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"  \n",
        "torch.device(device)  \n",
        "print(\"Current Device :\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read of multiple pickle files batchwise. Read the required file with input commentText and label. "
      ],
      "metadata": {
        "id": "2F8oZm8ZbfMC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzYzmAJdEahz"
      },
      "outputs": [],
      "source": [
        "def ReadData(path):\n",
        " path=path+\"*.pkl\" \n",
        " all_files = sorted(glob.glob(path))\n",
        " li = []\n",
        " labels=[]\n",
        " for filename in all_files:\n",
        "    df = pd.read_pickle(filename)\n",
        "    li.append(df)\n",
        "    labels.append(df.label)\n",
        "\n",
        " dataset = pd.concat(li, axis=0, ignore_index=True)\n",
        " labels=pd.concat(labels,axis=0,ignore_index=True)\n",
        " return dataset,labels "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKOehWR5HZrf"
      },
      "outputs": [],
      "source": [
        "def ExtractFeatures(dataset,labels,model_name):\n",
        "  model_pipeline=model(dataset,model_name)\n",
        "  model_pipeline.tokenize(dataset.commentText)\n",
        "  feature_df_mbert=model_pipeline.GetFeatures(dataset.commentText)\n",
        "  feature_df_mbert['label']=labels\n",
        "  return feature_df_mbert "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWv_E42nLY7M"
      },
      "outputs": [],
      "source": [
        "def SaveAsCsv(dataframe,modelname):\n",
        "    filename='CD_'+str(modelname)+'.csv'\n",
        "    dataframe.to_csv(filename,index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9onTtUYawYT"
      },
      "outputs": [],
      "source": [
        "def classify(dataframe,classifier):\n",
        "   classifier_class=classifiers(features=dataframe.iloc[:, :-1],label=dataframe.label)\n",
        "   test_labels=dataframe.label.tail(len(dataframe)-int(0.8*len(dataframe)))\n",
        "   if classifier == 'A':\n",
        "    accuracies,f1_scores,models,y=classifier_class.classify(svm=True,random_forest=True,xgboost=True,logistic_regression=True,ann=False)\n",
        "    for i in range(len(y)) :\n",
        "      print(models[i])\n",
        "      print(classification_report(test_labels, y[i], labels=[0,1]))\n",
        "    a,f,cm,y_pred=classifier_class.MajorityVotingClassifier(2)  \n",
        "    print(\"VC\")\n",
        "    print(classification_report(test_labels, y_pred, labels=[0,1]))   \n",
        "   elif classifier == 'LR':\n",
        "      accuracies,f1_scores,models,y=classifier_class.classify(svm=False,random_forest=False,xgboost=False,logistic_regression=True,ann=False)\n",
        "      print(classification_report(test_labels, y[0], labels=[0,1]))\n",
        "   elif classifier == 'RF':  \n",
        "      accuracies,f1_scores,models,y=classifier_class.classify(svm=False,random_forest=True,xgboost=False,logistic_regression=False,ann=False)\n",
        "      print(classification_report(test_labels, y[0], labels=[0,1]))\n",
        "   elif classifier == 'XGBOOST':\n",
        "      accuracies,f1_scores,models,y=classifier_class.classify(svm=False,random_forest=False,xgboost=True,logistic_regression=False,ann=False)\n",
        "      print(classification_report(test_labels, y[0], labels=[0,1]))\n",
        "   elif classifier == 'VC': \n",
        "      a,f,cm,y_pred=classifier_class.MajorityVotingClassifier(2)  \n",
        "      print(classification_report(test_labels, y_pred, labels=[0,1])) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ts6K45fCFhWh"
      },
      "outputs": [],
      "source": [
        "def main(model_name,classifier):\n",
        "    args = sys.argv[1:]\n",
        "    dataset,labels=ReadData('')\n",
        "    dataframe=ExtractFeatures(dataset.head(10),labels.head(10),model_name)\n",
        "    SaveAsCsv(dataframe,model)\n",
        "    classify(dataframe,classifier)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4YK9wUq1uuN"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main('mBert','A')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "experiment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}